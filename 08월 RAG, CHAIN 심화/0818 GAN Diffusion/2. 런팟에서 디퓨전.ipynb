{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071b85e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "runpod \n",
    "pip install genaibook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bec5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DDPMPipeline\n",
    "from genaibook.core import get_device\n",
    "\n",
    "# We can set the device to use our GPU or CPU\n",
    "device = get_device()\n",
    "\n",
    "# Load the pipeline\n",
    "image_pipe = DDPMPipeline.from_pretrained(\"google/ddpm-celebahq-256\")\n",
    "image_pipe.to(device)\n",
    "\n",
    "# Sample an image\n",
    "image_pipe().images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0895d2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genaibook.core import plot_noise_and_denoise\n",
    "image = torch.randn(4, 3, 256, 256).to(device)\n",
    "image_pipe.scheduler.set_timesteps(num_inference_steps=100)\n",
    "for i, t in enumerate(image_pipe.scheduler.timesteps):\n",
    "    with torch.inference_mode():\n",
    "        noise_pred = image_pipe.unet(image, t)[\"sample\"]\n",
    "    scheduler_output = image_pipe.scheduler.step(noise_pred, t, image)\n",
    "    image = scheduler_output.prev_sample\n",
    "    if i % 10 == 0 or i == len(image_pipe.scheduler.timesteps) - 1:\n",
    "        plot_noise_and_denoise(scheduler_output, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80870435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"huggan/smithsonian_butterflies_subset\", split=\"train\")\n",
    "\n",
    "from torchvision import transforms\n",
    "image_size = 384\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((image_size, image_size)),  # Resize\n",
    "        transforms.RandomHorizontalFlip(),  # Randomly flip (data augmentation)\n",
    "        transforms.ToTensor(),  # Convert to tensor (0, 1)\n",
    "        transforms.Normalize([0.5], [0.5]),  # Map to (-1, 1)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1440a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(examples):\n",
    "    examples = [preprocess(image) for image in examples[\"image\"]]\n",
    "    return {\"images\": examples}\n",
    "\n",
    "dataset.set_transform(transform)\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed65221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genaibook.core import show_images\n",
    "show_images(batch[\"images\"][:8] * 0.5 + 0.5)\n",
    "batch = next(iter(train_dataloader))\n",
    "from diffusers import DDPMScheduler\n",
    "\n",
    "scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=1000, beta_start=0.001, beta_end=0.02\n",
    ")\n",
    "\n",
    "timesteps = torch.linspace(0, 999, 8).long()\n",
    "x = batch[\"images\"][:8]\n",
    "noise = torch.rand_like(x)\n",
    "noised_x = scheduler.add_noise(x, noise, timesteps)\n",
    "show_images((noised_x * 0.5 + 0.5).clip(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d35a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "model = UNet2DModel(\n",
    "    in_channels=3,  # 3 channels for RGB images\n",
    "    sample_size=64,  # Specify our input size\n",
    "    # The number of channels per block affects the model size\n",
    "    block_out_channels=(64, 128, 256, 512),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\"AttnUpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e963f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    out = model(noised_x.to(device), timestep=timesteps.to(device)).sample\n",
    "\n",
    "from torchvision import transforms\n",
    "batch_size = 32\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((image_size, image_size)),  # Resize\n",
    "        transforms.RandomHorizontalFlip(),  # Randomly flip (data augmentation)\n",
    "        transforms.ToTensor(),  # Convert to tensor (0, 1)\n",
    "        transforms.Normalize([0.5], [0.5]),  # Map to (-1, 1)\n",
    "    ]\n",
    ")\n",
    "def transform(examples):\n",
    "    examples = [preprocess(image) for image in examples[\"image\"]]\n",
    "    return {\"images\": examples}\n",
    "\n",
    "dataset.set_transform(transform)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "from torch.nn import functional as F\n",
    "num_epochs = 50\n",
    "lr = 1e-4 \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "losses = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1621a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        clean_images = batch[\"images\"].to(device)\n",
    "        noise = torch.randn(clean_images.shape).to(device)\n",
    "        timesteps = torch.randint(\n",
    "                    0,\n",
    "                    scheduler.config.num_train_timesteps,\n",
    "                    (clean_images.shape[0],),\n",
    "                    device=device,\n",
    "                ).long()\n",
    "        noisy_images = scheduler.add_noise(clean_images, noise, timesteps)\n",
    "        noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    avg_loss = sum(losses[-len(train_dataloader) :]) / len(train_dataloader)\n",
    "    print(\n",
    "        f\"Finished epoch {epoch}. Average loss for this epoch: {avg_loss:05f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058b2472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.subplots(1, 2, figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Training step\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(400, len(losses)), losses[400:])\n",
    "plt.title(\"Training loss from step 400\")\n",
    "plt.xlabel(\"Training step\");\n",
    "\n",
    "sample = torch.randn(4, 3, 64, 64).to(device)\n",
    "show_images(sample.clip(-1, 1) * 0.5 + 0.5, nrows=1)\n",
    "\n",
    "for t in scheduler.timesteps:\n",
    "    with torch.inference_mode():\n",
    "        noise_pred = model(sample, t)[\"sample\"]\n",
    "        \n",
    "    sample = scheduler.step(noise_pred, t, sample).prev_sample\n",
    "show_images(sample.clip(-1, 1) * 0.5 + 0.5, nrows=1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
