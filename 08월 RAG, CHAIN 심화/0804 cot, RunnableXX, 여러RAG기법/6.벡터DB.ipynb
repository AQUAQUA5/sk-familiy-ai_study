{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4159dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "앞으로 사용하게 될 postgresql 설치하기 (벡터 db로 활용)\n",
    "----\n",
    "sudo apt install -y postgresql-common\n",
    "sudo /usr/share/postgresql-common/pgdg/apt.postgresql.org.sh\n",
    "\n",
    "sudo apt install curl ca-certificates\n",
    "sudo install -d /usr/share/postgresql-common/pgdg\n",
    "sudo curl -o /usr/share/postgresql-common/pgdg/apt.postgresql.org.asc --fail https://www.postgresql.org/media/keys/ACCC4CF8.asc\n",
    ". /etc/os-release\n",
    "sudo sh -c \"echo 'deb [signed-by=/usr/share/postgresql-common/pgdg/apt.postgresql.org.asc] https://apt.postgresql.org/pub/repos/apt $VERSION_CODENAME-pgdg main' > /etc/apt/sources.list.d/pgdg.list\"\n",
    "sudo apt update\n",
    "sudo apt -y install postgresql\n",
    "sudo service  postgresql start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd54f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "os.environ['LANGSMITH_PROJECT']  =\"skn15-1\"\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "llm.invoke(\"Hello, world!\")\n",
    "\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "loader = CSVLoader(file_path=\"./data/아리계곡_통합.csv\")\n",
    "docs = loader.load()\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "db = Chroma.from_documents(docs, embedding = embedding)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('''\\\n",
    "다음 문맥만을 고려해 질문에 답하세요.\n",
    "문맥: \"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "질문: {question}\n",
    "''')\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": retriever,\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n",
    "output = chain.invoke(\"가게 분위기는 어떤가?\")\n",
    "import pprint\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6f06eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "class QueryGenerationOutput(BaseModel):\n",
    "    queries: list[str] = Field(..., description=\"검색 쿼리 목록\")\n",
    "query_generation_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "질문에 대해 벡터 데이터베이스에서 관련 문서를 검색하기 위한\n",
    "3개의 서로 다른 검색 쿼리를 생성하세요.\n",
    "거리 기반 유사성 검색의 한계를 극복하기 위해\n",
    "사용자의 질문에 대해 여러 관점을 제공하는 것이 목표입니다.\n",
    "질문: {question}\n",
    "\"\"\")\n",
    "\n",
    "query_generation_chain = (\n",
    "    query_generation_prompt\n",
    "    | model.with_structured_output(QueryGenerationOutput)\n",
    "    | (lambda x: x.queries)\n",
    ")\n",
    "\n",
    "multi_query_rag_chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": query_generation_chain | retriever.map(),\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n",
    "multi_query_rag_chain.invoke(\"주위사람에게 추천할 의사는?\")\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def reciprocal_rank_fusion( retriever_outputs: list[list[Document]],  k: int = 60 ) -> list[str]:\n",
    "    # 각 문서의 콘텐츠(문자열)와 그 점수의 매핑을 저장하는 딕셔너리 준비\n",
    "    content_score_mapping = {}\n",
    "    # 검색 쿼리마다 반복\n",
    "    for docs in retriever_outputs:\n",
    "        # 검색 결과의 문서마다 반복\n",
    "        for rank, doc in enumerate(docs):\n",
    "            content = doc.page_content\n",
    "            # 처음 등장한 콘텐츠인 경우 점수를 0으로 초기화\n",
    "            if content not in content_score_mapping:\n",
    "                content_score_mapping[content] = 0\n",
    "            # (1 / (순위 + k)) 점수를 추가\n",
    "            content_score_mapping[content] += 1 / (rank + k)\n",
    "\n",
    "    # 점수가 큰 순서로 정렬\n",
    "    ranked = sorted(content_score_mapping.items(), key=lambda x: x[1], reverse=True)  # noqa\n",
    "    return [content for content, _ in ranked]\n",
    "\n",
    "rag_fusion_chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": query_generation_chain | retriever.map() | reciprocal_rank_fusion,\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n",
    "\n",
    "rag_fusion_chain.invoke(\"가게 분위기는?\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
