{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc8dc333",
   "metadata": {},
   "source": [
    "#### 실습은 런팟에서 진행하고 코드만 저장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a747d9b",
   "metadata": {},
   "source": [
    "- requirements.txt 파일 생성 \n",
    "transformers==4.44.2 \n",
    "datasets==2.18.0 \n",
    "accelerate==0.29.3 \n",
    "evaluate==0.4.1 \n",
    "bitsandbytes==0.43.1 \n",
    "huggingface_hub>=0.23.2 \n",
    "trl==0.8.6 \n",
    "peft==0.10.0\n",
    "scikit-learn \n",
    "wandb \n",
    "\n",
    "\n",
    "pip install -r requirements.txt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a4e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import trl\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"TRL version: {trl.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "#토큰값 넣기\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(\n",
    "  token=\"Your_Huggingface_API_KEY\",\n",
    "  add_to_git_credential=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "import json \n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from trl import (setup_chat_format, \n",
    "                 DataCollatorForCompletionOnlyLM, \n",
    "                 SFTTrainer)\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig, PeftConfig \n",
    "from transformers import (AutoTokenizer, \n",
    "                          AutoModelForCausalLM, \n",
    "                          TrainingArguments, \n",
    "                          BitsAndBytesConfig, \n",
    "                          pipeline, \n",
    "                          StoppingCriteria)\n",
    "\n",
    "model_id = \"google/gemma-2-9b-it\" \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation='eager'\n",
    "    # load_in_8bit=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3157dcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/MrBananaHuman/CounselGPT/main/total_kor_multiturn_counsel_bot.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7e294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./total_kor_multiturn_counsel_bot.jsonl\") as f:\n",
    "    original_jsonl_data = [json.loads(line) for line in f ]\n",
    "\n",
    "len(original_jsonl_data)\n",
    "test_data = original_jsonl_data [5085]\n",
    "\n",
    "speaker_dict = {'내담자' : 'user', '상담사' : 'assistant'}\n",
    "\n",
    "tmp = []\n",
    "for m in test_data:\n",
    "    tmp.append({'role' : speaker_dict[m['speaker']], 'content': m['utterance']})\n",
    "\n",
    "converted_messages = [{'role' : speaker_dict[m['speaker']], 'content': m['utterance']} for m in test_data]\n",
    "\n",
    "#assistant로 시작하는 경우 첫번째 메시지 제거 \n",
    "if converted_messages and converted_messages[0]['role'] == 'assistant':\n",
    "    converted_messages = converted_messages[1:]\n",
    "#user로 끝나는 경우 마지막 메시지들 제거 \n",
    "while converted_messages and converted_messages[-1]['role'] == 'user':\n",
    "    converted_messages = converted_messages[:-1]\n",
    "\n",
    "#대화의 화자가 연속적일때 병합 \n",
    "merged = []\n",
    "current_role = converted_messages[0]['role']\n",
    "current_content = converted_messages[0]['content']\n",
    "for message in converted_messages[1:]:\n",
    "    if message['role'] == current_role:\n",
    "        current_content = current_content + \" \" + message['content']\n",
    "    else:\n",
    "        merged.append({'role' : current_role, 'content' : current_content})\n",
    "        current_role = message['role']\n",
    "        current_content = message['content']\n",
    "merged.append({'role' : current_role , 'content' : current_content})\n",
    "\n",
    "converted_messages[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7915cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#위의 로직을 사용자 함수로 \n",
    "def preprocess_conversation(messages):\n",
    "    # speaker를 role로 변환\n",
    "    converted_messages = [{'role': speaker_dict[m['speaker']], 'content': m['utterance']} for m in messages]\n",
    "    \n",
    "    # assistant로 시작하는 경우 첫 메시지 제거\n",
    "    if converted_messages and converted_messages[0]['role'] == 'assistant':\n",
    "        converted_messages = converted_messages[1:]\n",
    "    \n",
    "    # user로 끝나는 경우 마지막 메시지들 제거\n",
    "    while converted_messages and converted_messages[-1]['role'] == 'user':\n",
    "        converted_messages = converted_messages[:-1]\n",
    "    \n",
    "    # 연속된 동일 역할의 메시지 병합\n",
    "    converted_messages = merge_consecutive_messages(converted_messages)\n",
    "    \n",
    "    # 대화가 비어있거나 홀수 개의 메시지만 남은 경우 처리\n",
    "    if not converted_messages or len(converted_messages) % 2 != 0:\n",
    "        return []\n",
    "    \n",
    "    return converted_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c35dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#대화가 연속된 동일 역할의 메시지 병합 함수\n",
    "def merge_consecutive_messages(messages):\n",
    "    if not messages:\n",
    "        return []\n",
    "    \n",
    "    merged = []\n",
    "    current_role = messages[0]['role']\n",
    "    current_content = messages[0]['content']\n",
    "    \n",
    "    for message in messages[1:]:\n",
    "        if message['role'] == current_role:\n",
    "            current_content += \" \" + message['content']\n",
    "        else:\n",
    "            merged.append({'role': current_role, 'content': current_content})\n",
    "            current_role = message['role']\n",
    "            current_content = message['content']\n",
    "    \n",
    "    merged.append({'role': current_role, 'content': current_content})\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1e4bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#위에서 만든 사용자 함수를 사용하여 전체 데이터에 적용할 함수 \n",
    "def transform_to_new_format(original_data):\n",
    "    transformed_data = []\n",
    "    for conversation in original_data:\n",
    "        processed_conversation = preprocess_conversation(conversation)\n",
    "        if processed_conversation:\n",
    "            transformed_data.append(processed_conversation)\n",
    "    return transformed_data\n",
    "\n",
    "result = transform_to_new_format(original_jsonl_data)\n",
    "\n",
    "with open(\"./train_dataset.jsonl\" , \"w\", encoding='utf-8') as f:\n",
    "    for conversation in result:\n",
    "        json_obj = {'messages' : conversation}\n",
    "        json.dump(json_obj, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "#ensure_ascii값을 True로 설정하면 아래와 같이 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d731c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#꼬리(마지막) 부분만 출력(마지막 한줄만 출력) (리눅스 명령어)\n",
    "tail -n 1 train_dataset.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484da9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"./train_dataset.jsonl\", split=\"train\")\n",
    "dataset\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        r=256,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"up_proj\",\n",
    "            \"o_proj\",\n",
    "            \"k_proj\",\n",
    "            \"down_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./model_output\", \n",
    "    num_train_epochs=1,          \n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,  \n",
    "    optim=\"adamw_torch_fused\",    \n",
    "    logging_steps=100,            \n",
    "    save_strategy=\"epoch\",        \n",
    "    learning_rate=2e-4,           \n",
    "    bf16=True,                    \n",
    "    tf32=True,                    \n",
    "    max_grad_norm=0.3,            \n",
    "    warmup_ratio=0.03,            \n",
    "    lr_scheduler_type=\"constant\", \n",
    "    push_to_hub=False,             \n",
    "    #report_to=\"wandb\",            \n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    max_seq_length=512,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddce412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#터미널에서 \n",
    "watch -n 1 nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
