{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f833795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms import v2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d357d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"dog.jpg\"\n",
    "path2 = \"dog2.jpg\"\n",
    "image = Image.open(path1)\n",
    "image2 = Image.open(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bf5a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compose\n",
    "import torchvision.transforms.v2 as v2\n",
    "import torch\n",
    "\n",
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Resize((224, 224))\n",
    "    ])\n",
    "transformed_image = transform(image)\n",
    "plt.imshow(transformed_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fb1b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomApply\n",
    "transform = v2.RandomApply([\n",
    "    v2.ColorJitter(brightness=0.5)\n",
    "    ], p=0.3)\n",
    "transformed_image = transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcbb6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomChoice\n",
    "transform = v2.RandomChoice([\n",
    "    v2.RandomHorizontalFlip(p=1.0),\n",
    "    v2.RandomVerticalFlip(p=1.0)\n",
    "    ])\n",
    "transformed_image = transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efa0a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToImage\n",
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize((128, 128))\n",
    "    ])\n",
    "transformed_image = transform(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3513939",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDtype\n",
    "import torch\n",
    "\n",
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "    ])\n",
    "transformed_image = transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1878068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToPILImage\n",
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToPILImage()\n",
    "    ])\n",
    "pil_image = transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce52516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "normalized_image = transform(image)\n",
    "plt.imshow(normalized_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474592b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize((32, 32))\n",
    "    ])\n",
    "resized_image = transform(image)\n",
    "plt.imshow(resized_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425caff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.CenterCrop(200)\n",
    "    ])\n",
    "cropped_image = transform(image)\n",
    "plt.imshow(cropped_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad509b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.RandomCrop(128)\n",
    "    ])\n",
    "cropped_image = transform(image)\n",
    "plt.imshow(cropped_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9038392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.RandomResizedCrop(\n",
    "        size=(224, 224),\n",
    "        scale=(0.5, 0.5),\n",
    "        ratio=(0.75, 1.25),\n",
    "        interpolation=InterpolationMode.BILINEAR,\n",
    "        antialias=True\n",
    "    )\n",
    "])\n",
    "cropped_image = transform(image)\n",
    "plt.imshow(cropped_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ffdc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Pad(padding=50, fill=128)\n",
    "    ])\n",
    "padded_image = transform(image)\n",
    "plt.imshow(padded_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fe27d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.RandomRotation(\n",
    "        degrees=45,\n",
    "        interpolation=InterpolationMode.BILINEAR,\n",
    "        expand=True,\n",
    "        fill=128\n",
    "        )\n",
    "    ])\n",
    "rotated_image = transform(image)\n",
    "plt.imshow(rotated_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d769159",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.RandomHorizontalFlip(p=0.7)\n",
    "    ])\n",
    "flipped_image = transform(image)\n",
    "plt.imshow(flipped_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04248f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.RandomVerticalFlip(p=0.3)\n",
    "    ])\n",
    "flipped_image = transform(image)\n",
    "plt.imshow(flipped_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ecec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.RandomAffine(\n",
    "        degrees=30,\n",
    "        translate=(0.5, 0.5),\n",
    "        scale=(0.8, 1.2),\n",
    "        shear=45,\n",
    "        interpolation=InterpolationMode.BILINEAR,\n",
    "        fill=100\n",
    "        )\n",
    "    ])\n",
    "affined_image = transform(image)\n",
    "plt.imshow(affined_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ed517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.RandomPerspective(\n",
    "        distortion_scale=0.7,\n",
    "        p=1.0,\n",
    "        fill=255\n",
    "        )\n",
    "    ])\n",
    "perspective_image = transform(image)\n",
    "plt.imshow(perspective_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ea273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ColorJitter(brightness=0.0, contrast=0.9, saturation=0.9, hue=0.5)\n",
    "    ])\n",
    "jittered_image = transform(image)\n",
    "plt.imshow(jittered_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9f86a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Grayscale(num_output_channels=1)\n",
    "    ])\n",
    "gray_image = transform(image)\n",
    "plt.imshow(gray_image.squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea01158",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.RandomInvert(p=1)\n",
    "    ])\n",
    "inverted_image = transform(image)\n",
    "plt.imshow(inverted_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7bf0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.RandomPosterize(bits=1, p=1)\n",
    "    ])\n",
    "posterized_image = transform(image)\n",
    "plt.imshow(posterized_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48960142",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.RandomSolarize(threshold=72, p=1)\n",
    "    ])\n",
    "solarized_image = transform(image)\n",
    "plt.imshow(solarized_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e66234",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.RandomAutocontrast(p=1)\n",
    "    ])\n",
    "autocontrasted_image = transform(image)\n",
    "plt.imshow(autocontrasted_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6fa164",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.RandomAutocontrast(p=1)\n",
    "    ])\n",
    "autocontrasted_image = transform(image)\n",
    "plt.imshow(autocontrasted_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d767e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.GaussianBlur(kernel_size=21, sigma=(5, 10))\n",
    "    ])\n",
    "blurred_image = transform(image)\n",
    "plt.imshow(blurred_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49819aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.RandomErasing(p=1, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=0)\n",
    "    ])\n",
    "erased_image = transform(image)\n",
    "plt.imshow(erased_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff1c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "path1 = \"dog.jpg\"\n",
    "path2 = \"dog2.jpg\"\n",
    "image1 = Image.open(path1)\n",
    "image2 = Image.open(path2)\n",
    "target1 = 1\n",
    "target2 = 0\n",
    "\n",
    "# 변환 정의 (Resize 추가)\n",
    "transform = v2.Compose([\n",
    "    v2.Resize((224, 224)),  # 모든 이미지를 224x224로 맞춤\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "# 이미지 리스트를 텐서로 변환\n",
    "images = [image1, image2]\n",
    "tensor_images = torch.stack([transform(img) for img in images])  # (2, 3, 224, 224)\n",
    "targets = torch.tensor([target1, target2])\n",
    "\n",
    "cutmix = v2.CutMix(num_classes=2, alpha=1.0)\n",
    "mixed_images, mixed_targets = cutmix(tensor_images, targets)\n",
    "\n",
    "img = mixed_images[0].permute(1, 2, 0).clamp(0, 1).numpy()  # (H, W, C), 값 범위 0~1\n",
    "plt.imshow(img)\n",
    "print(mixed_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee5ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "path1 = \"dog.jpg\"\n",
    "path2 = \"dog2.jpg\"\n",
    "image1 = Image.open(path1)\n",
    "image2 = Image.open(path2)\n",
    "target1 = 1\n",
    "target2 = 0\n",
    "\n",
    "# 변환 정의 (Resize 추가)\n",
    "transform = v2.Compose([\n",
    "    v2.Resize((224, 224)),  # 모든 이미지를 224x224로 맞춤\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "# 이미지 리스트를 텐서로 변환\n",
    "images = [image1, image2]\n",
    "tensor_images = torch.stack([transform(img) for img in images])  # (2, 3, 224, 224)\n",
    "targets = torch.tensor([target1, target2])\n",
    "\n",
    "mix = v2.MixUp(num_classes=10, alpha=1.0)\n",
    "mixed_images, mixed_targets = mix(tensor_images, targets)\n",
    "\n",
    "img = mixed_images[0].permute(1, 2, 0).clamp(0, 1).numpy()  # (H, W, C), 값 범위 0~1\n",
    "plt.imshow(img)\n",
    "print(mixed_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58100749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
