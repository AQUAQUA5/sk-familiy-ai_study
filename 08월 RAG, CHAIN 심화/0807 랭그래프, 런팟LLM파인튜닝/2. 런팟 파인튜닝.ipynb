{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4d1cc5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "LMM 파인튜닝\n",
    "1. 작은 데이터셋 학습 시 과적합 가능성 높음\n",
    "2. catastrophic forgetting 재앙적 망각 현상\n",
    "3. 학습 비용이 크다.\n",
    "4. 데이터 품질이 매우 중요하다. garbage in, garbage out\n",
    "\n",
    "허깅페이스 로그인\n",
    "1. 토큰 발급\n",
    "2. huggingface-cli login 또는 hf auth login << 터미널에 입력하여 허깅페이스 로그인\n",
    "3. \n",
    "from huggingface_hub import login\n",
    "login()\n",
    "으로도 가능\n",
    "\n",
    "토큰 직접입력도 가능\n",
    "pipe = pipeline(\"text-generation\", model=\"google/gemma-2b-it\", token=\"YOUR_TOKEN\")\n",
    "\n",
    "gemma 파인튜닝\n",
    "1. 런팟 48기가 gpu, 파이토치 2.1 디스크 둘다 200gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e67c358",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# BLEU\n",
    "- BLEU (Bilingual Evaluation Understudy)\n",
    "- 주로 기계 번역(Machine Translation)의 성능을 평가하기 위해 사용되는 지표\n",
    "- 생성된 텍스트(Candidate)가 정답 텍스트(Reference)와 얼마나 유사한지, 특히 **얼마나 많은 n-gram이 겹치는지**를 측정합니다.\n",
    "\n",
    "\n",
    "# 구현\n",
    "- **N-gram 정밀도(Precision) 기반:** BLEU는 **정밀도(Precision)** 를 중심으로 평가\n",
    "- 즉, 생성된 텍스트에 있는 n-gram이 정답 텍스트에 얼마나 많이 존재하는지를 측정\n",
    "- **수정된 N-gram 정밀도(Modified N-gram Precision):** 단순히 n-gram이 존재하는지 여부만 보는 것이 아니라, **정답 텍스트에서 n-gram이 나타나는 횟수 이상으로 중복 계산되지 않도록 제한**합니다. 이를 통해 생성된 텍스트가 특정 구절을 반복하여 BLEU 점수를 인위적으로 높이는 것을 방지합니다.\n",
    "- **길이 패널티(Brevity Penalty):** 생성된 텍스트가 정답 텍스트보다 **지나치게 짧을 경우 점수에 패널티를 부여**합니다. 이를 통해 생성된 텍스트가 너무 짧아서 정보를 충분히 담고 있지 않을 때 점수가 과도하게 높아지는 것을 방지합니다.\n",
    "- **일반적으로 1~4 gram을 사용**합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4afe205",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc10dcb1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d3cc98",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 런팟 실습 내용\n",
    "requirements.txt\n",
    "\n",
    "transformers \n",
    "datasets \n",
    "evaluate \n",
    "trl \n",
    "huggingface_hub \n",
    "accelerate \n",
    "wandb \n",
    "scikit-learn\n",
    "\n",
    "\n",
    "pip install -r requirements.txt \n",
    "\n",
    "pip install torch torchvision torchaudio -U \n",
    "\n",
    "pip uninstall trl \n",
    "\n",
    "pip install trl==0.19.1\n",
    "\n",
    "\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    pipeline, \n",
    "    Trainer\n",
    ")\n",
    "from transformers.integrations import WandbCallback\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "import evaluate\n",
    "\n",
    "model_name = \"google/gemma-2b-it\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "    use_cache=False,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation=\"eager\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "import datasets\n",
    "dataset = datasets.load_dataset(\"jaehy12/news3\")\n",
    "\n",
    "element = dataset['train'][1]\n",
    "\n",
    "def get_chat_format(example):\n",
    "    return [\n",
    "            {\"role\": \"user\", \"content\": f\"다음 텍스트를 한국어로 간단히 요약 및 관련 키워를 추출해주세요:\\n{example['original']}\"},\n",
    "            {\"role\": \"assistant\", \"content\": f\"한국어 요약:\\n{example['summary']}\"}\n",
    "        ]\n",
    "\n",
    "formatted = tokenizer.apply_chat_template(\n",
    "    get_chat_format(element), tokenize=False\n",
    ")\n",
    "\n",
    "\n",
    "input_text = \"\"\"다음 텍스트를 한국어로 간단히 요약해주세요:\\n부산의 한 왕복 2차선 도로에서 역주행 사고로 배달 오토바이 운전자인 고등학생이 숨지는 사고가 발생했다.\n",
    "유족은 '가해자가 사고 후 곧바로 신고하지 않고 늑장 대응해 피해를 키웠다'고 주장하고 있다.\n",
    "11일 부산진경찰서는 교통사고처리특례법(교통사고처리법)상 업무상 과실치사 혐의로 지난 3일 A(59)씨를 검찰에 불구속 송치했다고 밝혔다.\n",
    "A씨는 교통사고처리법상 12대 중과실에 해당되는 '중앙선 침범'으로 역주행 교통사고를 일으킨 혐의를 받는다.\n",
    "경찰에 따르면 스포츠유틸리티차량(SUV) 운전자 A씨는 5월 19일 밤 11시 50분쯤 부산진구 가야고가교 밑 도로에서 중앙선을 넘어 역주행으로 140m를 달려\n",
    "반대편 차선의 오토바이 운전자 조모(16)군을 들이받았다. 조군은 원동기장치자전거 면허를 취득한 상태였고 헬멧도 쓰고 있었지만 크게 다쳤다.\n",
    "사고 당일 수술을 받았으나 얼마 후 2차 뇌출혈로 뇌사 판정이 내려졌고, 사고 발생 약 한 달 만인 지난달 16일 끝내 사망했다.\n",
    "사고를 낸 A씨는 술을 마시거나 약물을 복용한 상태에서 운전하지는 않은 것으로 조사됐다.\n",
    "경찰 관계자는 'A씨가 자신이 정주행을 하고 오토바이가 역주행을 한 것으로 착각했다고 진술했다'고 설명했다.\"\"\"\n",
    "\n",
    "\n",
    "#키워드 추출만 시켰을 때 \n",
    "def change_inference_chat_format(input_text):\n",
    "    return [\n",
    "    {\"role\": \"user\", \"content\": f\"{input_text}\"},\n",
    "\t{\"role\": \"assistant\", \"content\": \"\"\"부산의 한 왕복 2차선 도로에서 역주행 사고로 배달 오토바이 운전자인 고등학생이 숨지는 사고가 발생했다.\n",
    "     유족은 '가해자가 사고 후 곧바로 신고하지 않고 늑장 대응해 피해를 키웠다'고 주장하고 있다.\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": \"중요한 키워드 5개를 뽑아주세요.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"\"}\n",
    "    ]\n",
    "prompt = change_inference_chat_format(input_text)\n",
    "\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    prompt, \n",
    "    tokenize=True, \n",
    "    add_generation_prompt=True, \n",
    "    return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.to(model.device), \n",
    "    max_new_tokens=256\n",
    "    )\n",
    "print(tokenizer.decode(\n",
    "    outputs[0], \n",
    "    skip_special_tokens=True\n",
    "    ))\n",
    "\n",
    "\n",
    "#한국어 요약만 시켰을 때\n",
    "def change_inference_chat_format(input_text):\n",
    "    return [\n",
    "    {\"role\": \"user\", \"content\": f\"{input_text}\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"한국어 요약:\\n\"}\n",
    "    ]\n",
    "prompt = change_inference_chat_format(input_text)\n",
    "\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(prompt,\n",
    "                                       tokenize=True, \n",
    "                                       add_generation_prompt=True,\n",
    "                                       return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(inputs, max_new_tokens=256, use_cache=True)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "#2가지를 동시에 요청하는 코드 \n",
    "def change_inference_chat_format(input_text):\n",
    "    return [\n",
    "    {\"role\": \"user\", \"content\": f\"다음 텍스트를 한국어로 간단히 요약하고, 관련된 5개의 키워를 추출해주세요:\\n{input_text}\"},\n",
    "\t{\"role\": \"assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "prompt = change_inference_chat_format(input_text)\n",
    "inputs = tokenizer.apply_chat_template(prompt, \n",
    "                                       tokenize=True, \n",
    "                                       add_generation_prompt=True, \n",
    "                                       return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(inputs, max_new_tokens=256, use_cache=True)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
    "\n",
    "\n",
    "def key_word_prompt(input_text, summary_text):\n",
    "    return [\n",
    "    {\"role\": \"user\", \"content\": f\"{input_text}\"},\n",
    "    {\"role\": \"assistant\", \"content\": f\"{summary_text}\"},\n",
    "    {\"role\": \"user\", \"content\": \"중요한 키워드 5개를 뽑아주세요.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"\"}\n",
    "    ]\n",
    "\n",
    "def extract_keywords_batch(batch):\n",
    "    prompts = [key_word_prompt(original, summary) for original, summary in zip(batch[\"original\"], batch[\"summary\"])]\n",
    "\n",
    "    generated_texts = pipe(prompts, max_new_tokens=150, return_full_text=False)\n",
    "    keywords = [gen_text[0][\"generated_text\"] for gen_text in generated_texts]\n",
    "    batch[\"keywords\"] = keywords\n",
    "    return batch\n",
    "\n",
    "\n",
    "sample_dataset = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "sample_dataset = sample_dataset.map(extract_keywords_batch, batched=True, batch_size=20)   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def chat_keyword_summary_format(example):\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": f\"다음 텍스트를 한국어로 간단히 요약 및 관련 키워를 추출해주세요:\\n{example['original']}\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"한국어 요약:{example['summary']}\\n키워드:{example['keywords']}\"}\n",
    "    ]\n",
    "\n",
    "formatted = tokenizer.apply_chat_template(\n",
    "    chat_keyword_summary_format(sample_dataset[0]), tokenize=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def tokenize(element):\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        chat_keyword_summary_format(element), tokenize=False\n",
    "    ) + EOS_TOKEN\n",
    "    outputs = tokenizer(formatted)\n",
    "    return {\n",
    "        \"input_ids\": outputs[\"input_ids\"],\n",
    "        \"attention_mask\": outputs[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "tokenized_sample_dataset = sample_dataset.map(tokenize)\n",
    "\n",
    "tokenized_sample_dataset = tokenized_sample_dataset.train_test_split(\n",
    "    test_size=0.1, \n",
    "    seed=42\n",
    ")\n",
    "\n",
    "response_template_ids = tokenizer.encode(\n",
    "    \"<start_of_turn>model\\n\", \n",
    "    add_special_tokens=False\n",
    "    )\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template_ids, tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "wandb.init(project=\"gemma-2B-it-Full-Fine-Tuning\", entity=\"Your_ID\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./keywords_gemma_results\",\n",
    "    # num_train_epochs=1, # 1epoch에 250step정도 진행함 \n",
    "    max_steps=800,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=0,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-4,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    report_to=\"wandb\",\n",
    "    )\n",
    "\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "acc = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        # 모델과 설정에 따라 logits에는 추가적인 텐서들이 포함될 수 있습니다.\n",
    "        # 예를 들어, past_key_values 같은 것들이 있을 수 있지만,\n",
    "        # logits는 항상 첫 번째 요소입니다.\n",
    "        logits = logits[0]\n",
    "    # 토큰 ID를 얻기 위해 argmax를 수행합니다.\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # preds는 labels와 같은 형태를 갖습니다.\n",
    "    # preprocess_logits_for_metrics에서 argmax(-1)가 계산된 후입니다.\n",
    "    # 하지만 우리는 labels를 한 칸 이동시켜야 합니다.\n",
    "    labels = labels[:, 1:]\n",
    "    preds = preds[:, :-1]\n",
    "\n",
    "    # -100은 DataCollatorForCompletionOnlyLM에서 사용되는 \n",
    "    # ignore_index의 기본값입니다.\n",
    "    mask = labels == -100\n",
    "    # -100을 토크나이저가 디코드할 수 있는 값으로 대체합니다.\n",
    "    labels[mask] = tokenizer.pad_token_id\n",
    "    preds[mask] = tokenizer.pad_token_id\n",
    "\n",
    "    # BLEU 점수는 텍스트를 입력으로 받기 때문에,\n",
    "    # 토큰 ID에서 텍스트로 변환해야 합니다.\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    bleu_score = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    \n",
    "    # accuracy는 정수 리스트를 입력으로 받습니다.\n",
    "    # 우리는 -100이 아닌 부분만 평가하고 싶기 때문에,\n",
    "    # 마스크의 부정(~)을 사용합니다.\n",
    "    accuracy = acc.compute(predictions=preds[~mask], references=labels[~mask])\n",
    "\n",
    "    return {**bleu_score, **accuracy}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    train_dataset=tokenized_sample_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_sample_dataset[\"test\"],\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[WandbCallback()]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
