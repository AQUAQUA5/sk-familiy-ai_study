{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afc62de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from konlpy.tag import Komoran\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2aa4092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ratings_test.txt', <http.client.HTTPMessage at 0x7fe2eee2a450>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "226edbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "705f31ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 리뷰 개수 : 150000\n",
      "테스트용 리뷰 개수 : 50000\n"
     ]
    }
   ],
   "source": [
    "print('훈련용 리뷰 개수 :',len(train_data)) # 훈련용 리뷰 개수 출력\n",
    "print('테스트용 리뷰 개수 :',len(test_data)) # 테스트용 리뷰 개수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4257fd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41439/2205600232.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_data['document'].replace('', np.nan, inplace=True)\n",
      "/tmp/ipykernel_41439/2205600232.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_data['document'].replace('', np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True)\n",
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\", regex=True)\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "train_data.dropna(inplace=True)\n",
    "\n",
    "test_data.drop_duplicates(subset = ['document'], inplace=True)\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True) \n",
    "test_data['document'] = test_data['document'].str.replace('^ +', \"\", regex=True)\n",
    "test_data['document'].replace('', np.nan, inplace=True)\n",
    "test_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c2223a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 리뷰 개수 : 148740\n",
      "테스트용 리뷰 개수 : 48852\n"
     ]
    }
   ],
   "source": [
    "print('훈련용 리뷰 개수 :',len(train_data)) # 훈련용 리뷰 개수 출력\n",
    "print('테스트용 리뷰 개수 :',len(test_data)) # 테스트용 리뷰 개수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbb38a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "komoran = Komoran()\n",
    "komoran.morphs(\"아 더빙 진짜 짜증나네요 목소리\")\n",
    "\n",
    "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b413be4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148740/148740 [00:41<00:00, 3609.67it/s]\n",
      "100%|██████████| 48852/48852 [00:16<00:00, 2918.90it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "for sentence in tqdm(train_data['document']):\n",
    "    tokenized_sentence = komoran.morphs(sentence) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(stopwords_removed_sentence)\n",
    "\n",
    "X_test = []\n",
    "for sentence in tqdm(test_data['document']):\n",
    "    tokenized_sentence = komoran.morphs(sentence) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(stopwords_removed_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38d1dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f200a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aae0d964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 단어수 : 47921\n"
     ]
    }
   ],
   "source": [
    "word_list = []\n",
    "for sent in X_train:\n",
    "    for word in sent:\n",
    "      word_list.append(word)\n",
    "\n",
    "word_counts = Counter(word_list)\n",
    "print('총 단어수 :', len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df970c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58421a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3\n",
    "total_cnt = len(word_counts) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8ade3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1894b072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 47921\n",
      "등장 빈도가 2번 이하인 희귀 단어의 수: 31908\n",
      "단어 집합에서 희귀 단어의 비율: 66.58458713298971\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 2.3946380069916438\n"
     ]
    }
   ],
   "source": [
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bc2079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {}\n",
    "word_to_index['<PAD>'] = 0\n",
    "word_to_index['<UNK>'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02fb036f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패딩 토큰과 UNK 토큰을 고려한 단어 집합의 크기 : 47923\n"
     ]
    }
   ],
   "source": [
    "for index, word in enumerate(vocab) :\n",
    "  word_to_index[word] = index + 2\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "print('패딩 토큰과 UNK 토큰을 고려한 단어 집합의 크기 :', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fd67d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_sequences(tokenized_X_data, word_to_index):\n",
    "  encoded_X_data = []\n",
    "  for sent in tokenized_X_data:\n",
    "    index_sequences = []\n",
    "    for word in sent:\n",
    "      try:\n",
    "          index_sequences.append(word_to_index[word])\n",
    "      except KeyError:\n",
    "          index_sequences.append(word_to_index['<UNK>'])\n",
    "    encoded_X_data.append(index_sequences)\n",
    "  return encoded_X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a9ae8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_X_train = texts_to_sequences(X_train, word_to_index)\n",
    "encoded_X_valid = texts_to_sequences(X_valid, word_to_index)\n",
    "encoded_X_test = texts_to_sequences(X_test, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0480a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 30\n",
    "def pad_sequences(sentences, max_len):\n",
    "  features = np.zeros((len(sentences), max_len), dtype=int)\n",
    "  for index, sentence in enumerate(sentences):\n",
    "    if len(sentence) != 0:\n",
    "      features[index, :len(sentence)] = np.array(sentence)[:max_len]\n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f165700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_X_train = pad_sequences(encoded_X_train, max_len=max_len)\n",
    "padded_X_valid = pad_sequences(encoded_X_valid, max_len=max_len)\n",
    "padded_X_test = pad_sequences(encoded_X_test, max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f00052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c45b4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.7.1+cu128  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e60d6df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_length)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_length, embedding_dim)\n",
    "\n",
    "        # LSTM은 (hidden state, cell state)의 튜플을 반환합니다\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)  # lstm_out: (batch_size, seq_length, hidden_dim), hidden: (1, batch_size, hidden_dim)\n",
    "\n",
    "        last_hidden = hidden.squeeze(0)  # (batch_size, hidden_dim)\n",
    "        logits = self.fc(last_hidden)  # (batch_size, output_dim)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51c67c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_label_tensor = torch.tensor(np.array(y_train))\n",
    "valid_label_tensor = torch.tensor(np.array(y_valid))\n",
    "test_label_tensor = torch.tensor(np.array(y_test))\n",
    "print(train_label_tensor[:5])\n",
    "\n",
    "encoded_train = torch.tensor(padded_X_train).to(torch.int64)\n",
    "train_dataset = TensorDataset(encoded_train, train_label_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=32)\n",
    "\n",
    "encoded_test = torch.tensor(padded_X_test).to(torch.int64)\n",
    "test_dataset = TensorDataset(encoded_test, test_label_tensor)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=1)\n",
    "\n",
    "encoded_valid = torch.tensor(padded_X_valid).to(torch.int64)\n",
    "valid_dataset = TensorDataset(encoded_valid, valid_label_tensor)\n",
    "valid_dataloader = DataLoader(valid_dataset, shuffle=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b20d4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassifier(\n",
       "  (embedding): Embedding(47923, 100)\n",
       "  (lstm): LSTM(100, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "output_dim = 2\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "model = TextClassifier(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e400b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9513d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(logits, labels):\n",
    "    # _, predicted = torch.max(logits, 1)\n",
    "    predicted = torch.argmax(logits, dim=1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    total = labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, valid_dataloader, criterion, device):\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 데이터로더로부터 배치 크기만큼의 데이터를 연속으로 로드\n",
    "        for batch_X, batch_y in valid_dataloader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            # 모델의 예측값\n",
    "            logits = model(batch_X)\n",
    "\n",
    "            # 손실을 계산\n",
    "            loss = criterion(logits, batch_y)\n",
    "\n",
    "            # 정확도와 손실을 계산함\n",
    "            val_loss += loss.item()\n",
    "            val_correct += calculate_accuracy(logits, batch_y) * batch_y.size(0)\n",
    "            val_total += batch_y.size(0)\n",
    "    val_accuracy = val_correct / val_total\n",
    "    val_loss /= len(valid_dataloader)\n",
    "\n",
    "    return val_loss, val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "124a83cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:\n",
      "Train Loss: 0.4784, Train Accuracy: 0.7457\n",
      "Validation Loss: 0.3804, Validation Accuracy: 0.8278\n",
      "Validation loss improved from inf to 0.3804. 체크포인트를 저장합니다.\n",
      "Epoch 2/5:\n",
      "Train Loss: 0.3315, Train Accuracy: 0.8542\n",
      "Validation Loss: 0.3598, Validation Accuracy: 0.8421\n",
      "Validation loss improved from 0.3804 to 0.3598. 체크포인트를 저장합니다.\n",
      "Epoch 3/5:\n",
      "Train Loss: 0.2658, Train Accuracy: 0.8894\n",
      "Validation Loss: 0.3632, Validation Accuracy: 0.8438\n",
      "Epoch 4/5:\n",
      "Train Loss: 0.2050, Train Accuracy: 0.9187\n",
      "Validation Loss: 0.3973, Validation Accuracy: 0.8410\n",
      "Epoch 5/5:\n",
      "Train Loss: 0.1519, Train Accuracy: 0.9428\n",
      "Validation Loss: 0.4543, Validation Accuracy: 0.8386\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_dataloader:\n",
    "        # Forward pass\n",
    "        batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n",
    "        # batch_X.shape == (batch_size, max_len)\n",
    "        logits = model(batch_X)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, batch_y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate training accuracy and loss\n",
    "        train_loss += loss.item()\n",
    "        train_correct += calculate_accuracy(logits, batch_y) * batch_y.size(0)\n",
    "        train_total += batch_y.size(0)\n",
    "\n",
    "    train_accuracy = train_correct / train_total\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    # Validation\n",
    "    val_loss, val_accuracy = evaluate(model, valid_dataloader, criterion, DEVICE)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # 검증 손실이 최소일 때 체크포인트 저장\n",
    "    if val_loss < best_val_loss:\n",
    "        print(f'Validation loss improved from {best_val_loss:.4f} to {val_loss:.4f}. 체크포인트를 저장합니다.')\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e4c9aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassifier(\n",
       "  (embedding): Embedding(47923, 100)\n",
       "  (lstm): LSTM(100, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 로드\n",
    "model.load_state_dict(torch.load('best_model_checkpoint.pth'))\n",
    "\n",
    "# 모델을 device에 올립니다.\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b59cbe4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model validation loss: 0.3598\n",
      "Best model validation accuracy: 0.8421\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터에 대한 정확도와 손실 계산\n",
    "val_loss, val_accuracy = evaluate(model, valid_dataloader, criterion, DEVICE)\n",
    "\n",
    "print(f'Best model validation loss: {val_loss:.4f}')\n",
    "print(f'Best model validation accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd3d0e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_tag = {0 : '부정', 1 : '긍정'}\n",
    "\n",
    "def predict(text, model, word_to_index, index_to_tag):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize the input text\n",
    "    tokens = komoran.morphs(text) # 토큰화\n",
    "    tokens = [word for word in tokens if not word in stopwords] # 불용어 제거\n",
    "    token_indices = [word_to_index.get(token, 1) for token in tokens]\n",
    "\n",
    "    # Convert tokens to tensor\n",
    "    input_tensor = torch.tensor([token_indices], dtype=torch.long).to(DEVICE)  # (1, seq_length)\n",
    "\n",
    "    # Pass the input tensor through the model\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)  # (1, output_dim)\n",
    "\n",
    "    # Get the predicted class index\n",
    "    predicted_index = torch.argmax(logits, dim=1)\n",
    "\n",
    "    # Convert the predicted index to its corresponding tag\n",
    "    predicted_tag = index_to_tag[predicted_index.item()]\n",
    "\n",
    "    return predicted_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "200317b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정\n"
     ]
    }
   ],
   "source": [
    "text = \"점심 겁나 맛있네\"\n",
    "print(predict(text, model, word_to_index, index_to_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1593b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_serv = {'word_to_index' : word_to_index, \n",
    "              'index_to_tag' : index_to_tag}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9b5092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open(\"model_serv.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_serv, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365d4af4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnn3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
