{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9d975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableMap\n",
    "uppercase = RunnableLambda(lambda x : x.upper(), name='to_upper')\n",
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model='gpt-4.1-2025-04-14')\n",
    "chain  = uppercase | model\n",
    "chain.invoke(\"Hi\")\n",
    "\n",
    "format_prompt = RunnableLambda(lambda x : f\"한국어로 번역할 것 : {x}\")\n",
    "pipeline = RunnableMap(\n",
    "    {\n",
    "    'original' : RunnablePassthrough(),\n",
    "    'translated' : format_prompt | model\n",
    "    }\n",
    ")\n",
    "pipeline.invoke(\"Nice to meet you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bbe062",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552b32f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain_experimental\n",
    "\n",
    "\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "text = \"\"\"토트넘 홋스퍼는 지난 31일 오후 8시 30분(한국시간) 홍콩 카이탁에 위치한 카이탁 스포츠 타운에서 열린 프리시즌 아시아 투어 친선경기에서 아스널에 1-0으로 승리했다. 이로써 토트넘은 최고의 분위기 속, 한국으로 향해 오는 3일 서울월드컵경기장에서 뉴캐슬 유나이티드와 맞대결을 펼친다.\n",
    "\n",
    "\n",
    "총력을 다하겠다고 선언한 토마스 프랭크 감독. 손흥민을 벤치로 내렸다. 토트넘은 오도베르, 히샬리송, 쿠두스, 베리발, 사르, 벤탄쿠르, 스펜스, 반 더 벤, 로메로, 포로, 비카리오로 선발 라인업을 꾸렸다. 손흥민과 양민혁은 벤치에서 경기를 지켜봤다.\n",
    "\n",
    "\n",
    "토트넘은 아스널을 상대로 단단한 짜임새를 보여줬다. 오도베르와 쿠두스가 양쪽 측면에서 활발하게 공격적인 움직임을 가져갔고, 수비는 아스널의 공격을 매번 차단했다. 빌드업도 인상적이었다. 토트넘은 전반 44분 상대의 실책을 놓치지 않은 사르가 골키퍼가 나온 것을 본 이후, 그대로 중앙선 부근에서 초장거리 원더골을 a성공시키며 1-0으로 전반을 마무리했다.\n",
    "\n",
    "\n",
    "후반에도 아스널의 맹공을 잘 지켜낸 토트넘이다. 토트넘은 라인을 내리고 수비에 집중했다. 아스널은 점유율을 높이며 박스 안으로 공을 투입했지만, 결정적인 슈팅이 나오지는 않았다. 손흥민은 후반 33분 그라운드를 밟았다. 후반 38분 역습 상황, 단독 드리블 돌파 이후 침투 패스를 넣으며 가벼운 몸놀림을 보여줬다. 후반 추가시간 종료 직전에도 스프린트에 이은 날카로운 왼발 크로스를 올리기도 했다.\n",
    "\n",
    "\n",
    "이변은 없었다. 경기는 토트넘의 1-0 승리로 막을 내렸다. 경기 직후 주최 측에서 준비한 2025 홍콩 투어 기념 시상식이 진행됐다. 흥미로운 장면이 포착됐다. 손흥민이 주장 완장을 히샬리송에게 채워주는 장면이었다. 아직 프랭크 감독 체제 '공식 주장'이 확정되지 않았기에, 손흥민은 경험을 해보라는 식으로 웃으며 히샬리송에게 완장을 건넸다.\n",
    "\n",
    "\n",
    "히샬리송도 환하게 웃으며 손흥민을 포옹했다. 이후 히샬리송은 몇 분간 완장을 차고 있었다. 시간이 흐르고 주최 측에서 세리머니를 위한 시상대를 준비했다. 시상대 앞에 선수들이 모여 들었는데, 중앙에는 역시 손흥민이 있었다. 손흥민의 왼팔에 다시 주장 완장이 채워져 있었다.\n",
    "\n",
    "\n",
    "권위가 있는 대회는 아니지만, 주최 측에서 준비한 승리팀 기념 트로피를 들고 세리머니를 펼쳐야 했기 때문이었다. 이에 히샬리송은 전임 주장이자, 토트넘 레전드인 손흥민에게 다시 완장을 건넨 것으로 보인다. 이후 손흥민은 멋쩍게 웃으며 선수들과 함께 트로피를 들어 올렸고, 아스널을 꺾은 기쁨을 누렸다.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "text_splitter = SemanticChunker(\n",
    "    OpenAIEmbeddings(),\n",
    "    breakpoint_threshold_type=\"percentile\", # 백분위수 기준\n",
    "    breakpoint_threshold_amount=70, # 임계값 70%\n",
    ")\n",
    "\n",
    "\n",
    "docs = text_splitter.create_documents([text])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "text_splitter = SemanticChunker(\n",
    "    OpenAIEmbeddings(),\n",
    "    breakpoint_threshold_type=\"standard_deviation\", # 표준편차 기준\n",
    "    breakpoint_threshold_amount=1.25, # 임계값 1.25\n",
    ")\n",
    "\n",
    "\n",
    "docs2 = text_splitter.create_documents([text])\n",
    "\n",
    "\n",
    "\n",
    "text_splitter = SemanticChunker(\n",
    "    OpenAIEmbeddings(),\n",
    "    breakpoint_threshold_type=\"interquartile\", # 사분위수 범위 기준\n",
    "    breakpoint_threshold_amount=0.5, # 임계값 0.5\n",
    ")\n",
    "\n",
    "\n",
    "# 텍스트를 document 객체로 분할\n",
    "docs3 = text_splitter.create_documents([text])\n",
    "\n",
    "\n",
    "from langchain_text_splitters import Language,  RecursiveCharacterTextSplitter\n",
    "\n",
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "\n",
    "hello_world()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "markdown_document = \"\"\"\n",
    "# Title\n",
    "\n",
    "\n",
    "## 1. SubTitle\n",
    "\n",
    "\n",
    "Hi this is Jim\n",
    "\n",
    "\n",
    "Hi this is Joe\n",
    "\n",
    "\n",
    "### 1-1. Sub-SubTitle\n",
    "\n",
    "\n",
    "Hi this is Lance\n",
    "\n",
    "\n",
    "## 2. Baz\n",
    "\n",
    "\n",
    "Hi this is Molly\n",
    "\"\"\"\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"), # 헤더 레벨 1은 '#'로 표시되며, 'Header 1'이라는 이름을 가집니다.\n",
    "    (\"##\", \"Header 2\"), # 헤더 레벨 2는 '##'로 표시되며, 'Header 2'라는 이름을 가집니다.\n",
    "    (\"###\", \"Header 3\"), # 헤더 레벨 3은 '###'로 표시되며, 'Header 3'이라는 이름을 가집니다.\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "\n",
    "\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "\n",
    "for header in md_header_splits:\n",
    "    print(f\"{header.page_content}\")\n",
    "    print(f\"{header.metadata}\", end=\"\\n=====================\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    MarkdownHeaderTextSplitter,\n",
    ")\n",
    "\n",
    "\n",
    "markdown_document_long = \"\"\"\n",
    "# Intro\n",
    "\n",
    "\n",
    "## History\n",
    "\n",
    "\n",
    "Markdown is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.\n",
    "\n",
    "\n",
    "Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.\n",
    "\n",
    "\n",
    "## Rise and divergence\n",
    "\n",
    "\n",
    "As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for\n",
    "\n",
    "\n",
    "additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks.\n",
    "\n",
    "\n",
    "#### Standardization\n",
    "\n",
    "\n",
    "From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterized as a standardization effort.\n",
    "\n",
    "\n",
    "# Implementations\n",
    "\n",
    "\n",
    "Implementations of Markdown are available for over a dozen programming languages.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "headers_to_split_on = [(\"#\", \"Header 1\")]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document_long)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "chunk_size = 200 # 분할된 청크 크기를 지정\n",
    "chunk_overlap = 20 # 분할된 청크 간의 중복되는 문자 수를 지정\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "\n",
    "splits = text_splitter.split_documents(md_header_splits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "\n",
    "html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "<div>\n",
    "<h1>헤더 1</h1>\n",
    "<p>헤더 1에 포함된 본문</p>\n",
    "<div>\n",
    "<h2>헤더 2-1 제목</h2>\n",
    "<p>헤더 2-1에 포함된 본문</p>\n",
    "<h3>헤더 3-1 제목</h3>\n",
    "<p>헤더 3-1에 포함된 본문</p>\n",
    "<h3>헤더 3-2 제목</h3>\n",
    "<p>헤더 3-2에 포함된 본문</p>\n",
    "</div>\n",
    "<div>\n",
    "<h2>헤더 2-2 제목2</h2>\n",
    "<p>헤더 2-2에 포함된 본문</p>\n",
    "</div>\n",
    "<br>\n",
    "<p>마지막 내용</p>\n",
    "</div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# HTML 헤더 태그와 헤더의 이름을 지정\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "\n",
    "# HTMLHeaderTextSplitter 초기화\n",
    "html_splitter = HTMLHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "\n",
    "\n",
    "# HTML 문서 구조를 탐색하며 설정된 헤더 태그를 찾고 각 헤더 아래 내용을 분할\n",
    "html_header_splits = html_splitter.split_text(html_string)\n",
    "\n",
    "\n",
    "# 분할된 결과 출력\n",
    "for header in html_header_splits:\n",
    "    print(f\"{header.page_content}\")\n",
    "    print(f\"{header.metadata}\", end=\"\\n=====================\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "\n",
    "# JSON 데이터를 로드 (예시: LangSmith OpenAPI 스펙)\n",
    "json_data = requests.get(\"https://api.smith.langchain.com/openapi.json\").json()\n",
    "\n",
    "\n",
    "# RecursiveJsonSplitter 초기화 (최대 300자 크기의 청크로 분할)\n",
    "splitter = RecursiveJsonSplitter(max_chunk_size=300)\n",
    "\n",
    "\n",
    "# 1. JSON 데이터를 재귀적으로 분할하여 JSON 조각 리스트 생성\n",
    "json_chunks = splitter.split_json(json_data=json_data)\n",
    "# 2. JSON 데이터를 기반으로 Document 형식의 문서 생성\n",
    "docs = splitter.create_documents(texts=[json_data])\n",
    "# 3. JSON 데이터를 문자열로 변환한 후 max_chunk_size에 맞춰 청크 단위로 나눔\n",
    "texts = splitter.split_text(json_data=json_data)\n",
    "\n",
    "\n",
    "# 첫 번째 document 객체의 내용과 첫 번째 문자열 청크 출력\n",
    "print(docs)\n",
    "print(\"===\" * 20)\n",
    "print(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3096cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pdfplumber\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "\n",
    "loader = PDFPlumberLoader(\"./data/galaxy-s25-user-guide-SM-S93X-15-Korean.pdf\")\n",
    "\n",
    "text_spliter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "\n",
    "split_documents = text_spliter.split_documents(docs)\n",
    "\n",
    "\n",
    "#임베딩딩\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "retriever.invoke(\"전원 끄기\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rag_chatbot.py\n",
    "\n",
    "streamlit run rag_chatbot.py\n",
    "\n",
    "-----\n",
    "import streamlit as st\n",
    "from langchain_core.messages.chat import ChatMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableMap\n",
    "from langchain_ollama import ChatOllama\n",
    "import os\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_core.prompts import loading\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if os.path.isdir(\"./mycache\") == False:\n",
    "    os.mkdir(\"./mycache\")\n",
    "\n",
    "\n",
    "if os.path.isdir(\"./mycache/files\") == False:\n",
    "    os.mkdir(\"./mycache/files\")\n",
    "   \n",
    "if os.path.isdir(\"./mycache/embedding\") == False:\n",
    "    os.mkdir(\"./mycache/embedding\")\n",
    "   \n",
    "store = LocalFileStore(\"./mycache/embedding\")\n",
    "\n",
    "\n",
    "if 'chain' not in st.session_state:\n",
    "    st.session_state['chain'] = None\n",
    "\n",
    "\n",
    "st.title(\"RAG 기반 챗봇\")\n",
    "\n",
    "\n",
    "with st.sidebar:\n",
    "    uploaded_file = st.file_uploader(\"파일 업로드\", type=['pdf'])\n",
    "\n",
    "\n",
    "@st.cache_resource(show_spinner=\"업로드 파일 처리중 기다리세요\")\n",
    "def processing(file):\n",
    "    file_contents = file.read()\n",
    "    #파일 저장\n",
    "    with open(f\"./mycache/files/{file.name}\", \"wb\") as f:\n",
    "        f.write(file_contents)\n",
    "\n",
    "\n",
    "    #파일 임베딩\n",
    "    # insert your code\n",
    "    loader = PDFPlumberLoader(f\"./mycache/files/{file.name}\")\n",
    "    docs = loader.load()\n",
    "    text_spliter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "    split_documents = text_spliter.split_documents(docs)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "   \n",
    "    cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "                        underlying_embeddings=embeddings, # 기본 임베딩 모델 지정\n",
    "                        document_embedding_cache=store # 로컬 저장소 지정\n",
    "                    )\n",
    "    vectorstore = FAISS.from_documents(documents=split_documents, embedding=cached_embedder)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "    #######\n",
    "    return retriever\n",
    "\n",
    "\n",
    "join_docs = RunnableLambda(\n",
    "    lambda docs: \"\\n\".join(doc.page_content for doc in docs)\n",
    ")\n",
    "\n",
    "\n",
    "def create_chain(retriever):\n",
    "    prompt_text = {'_type': 'prompt',\n",
    "                'template': \"You are an assistant for question-answering tasks. \\nUse the following pieces of retrieved `information` to answer the question. \\nIf you don't know the answer, just say that you don't know. \\nAnswer in Korean.\\n\\n<information>\\n{context} \\n</information>\\n\\n#Question: \\n{question}\\n\\n#Answer:\\n\",\n",
    "                'input_variables': ['question', 'context']}\n",
    "    prompt = loading.load_prompt_from_config(prompt_text)\n",
    "    # llm = ChatOllama(model='gemma:7b', temperature=0)\n",
    "    llm = ChatOpenAI( model='gpt-4.1-2025-04-14', temperature=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    chain = (RunnableMap({\"context\" : retriever  | join_docs, \"question\" : RunnablePassthrough() })\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if uploaded_file:\n",
    "    retriever = processing(uploaded_file)\n",
    "    chain = create_chain(retriever)\n",
    "    st.session_state['chain'] = chain\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "user_input = st.chat_input(\"질문을 하세요\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if user_input:\n",
    "    chain = st.session_state['chain']\n",
    "\n",
    "\n",
    "    if chain is not None:\n",
    "        st.chat_message(\"user\").write(user_input)\n",
    "        response = chain.stream(user_input)\n",
    "\n",
    "\n",
    "        with st.chat_message('assistant'):\n",
    "            container = st.empty()\n",
    "\n",
    "\n",
    "            ai_answer = \"\"\n",
    "            for token in response:\n",
    "                ai_answer += token\n",
    "                container.markdown(ai_answer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "response = client.images.generate(\n",
    "    model='dall-e-3',\n",
    "    prompt=\"날씬한 돼지 그려줘\",\n",
    "    size=\"1024x1024\",\n",
    "    quality = 'standard',\n",
    "    response_format='b64_json',\n",
    "    n=1\n",
    ")\n",
    "\n",
    "\n",
    "result = response.model_dump()\n",
    "img = response.data[0]\n",
    "\n",
    "\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "img_data = base64.b64decode(img.b64_json)\n",
    "img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "\n",
    "---------------------------------------\n",
    "from openai import OpenAI\n",
    "import streamlit as st\n",
    "from langchain_core.messages.chat import ChatMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableMap\n",
    "from langchain_ollama import ChatOllama\n",
    "import os\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_core.prompts import loading\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "if os.path.isdir(\"./mycache\") == False:\n",
    "    os.mkdir(\"./mycache\")\n",
    "\n",
    "\n",
    "if os.path.isdir(\"./mycache/files\") == False:\n",
    "    os.mkdir(\"./mycache/files\")\n",
    "   \n",
    "if os.path.isdir(\"./mycache/embedding\") == False:\n",
    "    os.mkdir(\"./mycache/embedding\")\n",
    "   \n",
    "store = LocalFileStore(\"./mycache/embedding\")\n",
    "\n",
    "\n",
    "if 'chain' not in st.session_state:\n",
    "    st.session_state['chain'] = None\n",
    "\n",
    "\n",
    "st.title(\"RAG 기반 챗봇\")\n",
    "\n",
    "\n",
    "with st.sidebar:\n",
    "    uploaded_file = st.file_uploader(\"파일 업로드\", type=['pdf'])\n",
    "\n",
    "\n",
    "@st.cache_resource(show_spinner=\"업로드 파일 처리중 기다리세요\")\n",
    "def processing(file):\n",
    "    file_contents = file.read()\n",
    "    #파일 저장\n",
    "    with open(f\"./mycache/files/{file.name}\", \"wb\") as f:\n",
    "        f.write(file_contents)\n",
    "\n",
    "\n",
    "    #파일 임베딩\n",
    "    # insert your code\n",
    "    loader = PDFPlumberLoader(f\"./mycache/files/{file.name}\")\n",
    "    docs = loader.load()\n",
    "    text_spliter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "    split_documents = text_spliter.split_documents(docs)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "   \n",
    "    cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "                        underlying_embeddings=embeddings, # 기본 임베딩 모델 지정\n",
    "                        document_embedding_cache=store # 로컬 저장소 지정\n",
    "                    )\n",
    "    vectorstore = FAISS.from_documents(documents=split_documents, embedding=cached_embedder)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "    #######\n",
    "    return retriever\n",
    "\n",
    "\n",
    "join_docs = RunnableLambda(\n",
    "    lambda docs: \"\\n\".join(doc.page_content for doc in docs)\n",
    ")\n",
    "\n",
    "\n",
    "def create_chain(retriever):\n",
    "    prompt_text = {'_type': 'prompt',\n",
    "                'template': \"You are an assistant for question-answering tasks. \\nUse the following pieces of retrieved `information` to answer the question. \\nIf you don't know the answer, just say that you don't know. \\nAnswer in Korean.\\n\\n<information>\\n{context} \\n</information>\\n\\n#Question: \\n{question}\\n\\n#Answer:\\n\",\n",
    "                'input_variables': ['question', 'context']}\n",
    "    prompt = loading.load_prompt_from_config(prompt_text)\n",
    "    # llm = ChatOllama(model='gemma:7b', temperature=0)\n",
    "    llm = ChatOpenAI( model='gpt-4.1-2025-04-14', temperature=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    chain = (RunnableMap({\"context\" : retriever  | join_docs, \"question\" : RunnablePassthrough() })\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if uploaded_file:\n",
    "    retriever = processing(uploaded_file)\n",
    "    chain = create_chain(retriever)\n",
    "    st.session_state['chain'] = chain\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "options = (\"질문하기\", \"이미지 생성\")\n",
    "\n",
    "\n",
    "selected_radio = st.radio(\n",
    "    '다음 중 하나 선택하세요',\n",
    "    options\n",
    ")\n",
    "\n",
    "\n",
    "if selected_radio == \"질문하기\":\n",
    "\n",
    "\n",
    "    user_input = st.chat_input(\"질문을 하세요\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if user_input:\n",
    "        chain = st.session_state['chain']\n",
    "\n",
    "\n",
    "        if chain is not None:\n",
    "            st.chat_message(\"user\").write(user_input)\n",
    "            response = chain.stream(user_input)\n",
    "\n",
    "\n",
    "            with st.chat_message('assistant'):\n",
    "                container = st.empty()\n",
    "\n",
    "\n",
    "                ai_answer = \"\"\n",
    "                for token in response:\n",
    "                    ai_answer += token\n",
    "                    container.markdown(ai_answer)\n",
    "elif selected_radio == \"이미지 생성\":\n",
    "    generate_input = st.chat_input(\"생성하고 싶은 이미지를 설명해주세요\")\n",
    "\n",
    "\n",
    "    if generate_input:\n",
    "\n",
    "\n",
    "        response = client.images.generate(\n",
    "                    model='dall-e-3',\n",
    "                    prompt=generate_input,\n",
    "                    size=\"1024x1024\",\n",
    "                    quality = 'standard',\n",
    "                    response_format='b64_json',\n",
    "                    n=1\n",
    "                )\n",
    "        result = response.model_dump()\n",
    "        img = response.data[0]\n",
    "        img_data = base64.b64decode(img.b64_json)\n",
    "        img = Image.open(io.BytesIO(img_data))\n",
    "        st.image(img)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---------\n",
    "from openai import OpenAI\n",
    "import streamlit as st\n",
    "from langchain_core.messages.chat import ChatMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableMap\n",
    "from langchain_ollama import ChatOllama\n",
    "import os\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_core.prompts import loading\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "if os.path.isdir(\"./mycache\") == False:\n",
    "    os.mkdir(\"./mycache\")\n",
    "\n",
    "\n",
    "if os.path.isdir(\"./mycache/files\") == False:\n",
    "    os.mkdir(\"./mycache/files\")\n",
    "   \n",
    "if os.path.isdir(\"./mycache/embedding\") == False:\n",
    "    os.mkdir(\"./mycache/embedding\")\n",
    "   \n",
    "store = LocalFileStore(\"./mycache/embedding\")\n",
    "\n",
    "\n",
    "if 'chain' not in st.session_state:\n",
    "    st.session_state['chain'] = None\n",
    "\n",
    "\n",
    "st.title(\"RAG 기반 챗봇\")\n",
    "\n",
    "\n",
    "with st.sidebar:\n",
    "    uploaded_file = st.file_uploader(\"파일 업로드\", type=['pdf', 'txt'])\n",
    "\n",
    "\n",
    "@st.cache_resource(show_spinner=\"업로드 파일 처리중 기다리세요\")\n",
    "def processing(file):\n",
    "    if file.name.split(\".\")[-1] == \"pdf\":\n",
    "        file_contents = file.read()\n",
    "        #파일 저장\n",
    "        with open(f\"./mycache/files/{file.name}\", \"wb\") as f:\n",
    "            f.write(file_contents)\n",
    "\n",
    "\n",
    "        #파일 임베딩\n",
    "        # insert your code\n",
    "        loader = PDFPlumberLoader(f\"./mycache/files/{file.name}\")\n",
    "        docs = loader.load()\n",
    "        text_spliter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "        split_documents = text_spliter.split_documents(docs)\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "       \n",
    "        cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "                            underlying_embeddings=embeddings, # 기본 임베딩 모델 지정\n",
    "                            document_embedding_cache=store # 로컬 저장소 지정\n",
    "                        )\n",
    "        vectorstore = FAISS.from_documents(documents=split_documents, embedding=cached_embedder)\n",
    "        retriever = vectorstore.as_retriever()\n",
    "           \n",
    "    elif file.name.split(\".\")[-1] == \"txt\":\n",
    "        file_contents = file.read()\n",
    "        #파일 저장\n",
    "        with open(f\"./mycache/files/{file.name}\", \"w\", encoding='utf-8') as f:\n",
    "            f.write(file_contents.decode())\n",
    "        text = file_contents.decode()\n",
    "        text_splitter = SemanticChunker(\n",
    "                OpenAIEmbeddings(),\n",
    "                breakpoint_threshold_type=\"percentile\", # 백분위수 기준\n",
    "                breakpoint_threshold_amount=70, # 임계값 70%\n",
    "            )\n",
    "        split_documents = text_splitter.create_documents([text])\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "       \n",
    "        cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "                            underlying_embeddings=embeddings, # 기본 임베딩 모델 지정\n",
    "                            document_embedding_cache=store # 로컬 저장소 지정\n",
    "                        )\n",
    "        vectorstore = FAISS.from_documents(documents=split_documents, embedding=cached_embedder)\n",
    "        retriever = vectorstore.as_retriever()\n",
    "   \n",
    "    #######\n",
    "    return retriever\n",
    "\n",
    "\n",
    "join_docs = RunnableLambda(\n",
    "    lambda docs: \"\\n\".join(doc.page_content for doc in docs)\n",
    ")\n",
    "\n",
    "\n",
    "def create_chain(retriever):\n",
    "    prompt_text = {'_type': 'prompt',\n",
    "                'template': \"You are an assistant for question-answering tasks. \\nUse the following pieces of retrieved `information` to answer the question. \\nIf you don't know the answer, just say that you don't know. \\nAnswer in Korean.\\n\\n<information>\\n{context} \\n</information>\\n\\n#Question: \\n{question}\\n\\n#Answer:\\n\",\n",
    "                'input_variables': ['question', 'context']}\n",
    "    prompt = loading.load_prompt_from_config(prompt_text)\n",
    "    # llm = ChatOllama(model='gemma:7b', temperature=0)\n",
    "    llm = ChatOpenAI( model='gpt-4.1-2025-04-14', temperature=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    chain = (RunnableMap({\"context\" : retriever  | join_docs, \"question\" : RunnablePassthrough() })\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if uploaded_file:\n",
    "    retriever = processing(uploaded_file)\n",
    "    chain = create_chain(retriever)\n",
    "    st.session_state['chain'] = chain\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "options = (\"질문하기\", \"이미지 생성\")\n",
    "\n",
    "\n",
    "selected_radio = st.radio(\n",
    "    '다음 중 하나 선택하세요',\n",
    "    options\n",
    ")\n",
    "\n",
    "\n",
    "if selected_radio == \"질문하기\":\n",
    "\n",
    "\n",
    "    user_input = st.chat_input(\"질문을 하세요\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if user_input:\n",
    "        chain = st.session_state['chain']\n",
    "\n",
    "\n",
    "        if chain is not None:\n",
    "            st.chat_message(\"user\").write(user_input)\n",
    "            response = chain.stream(user_input)\n",
    "\n",
    "\n",
    "            with st.chat_message('assistant'):\n",
    "                container = st.empty()\n",
    "\n",
    "\n",
    "                ai_answer = \"\"\n",
    "                for token in response:\n",
    "                    ai_answer += token\n",
    "                    container.markdown(ai_answer)\n",
    "elif selected_radio == \"이미지 생성\":\n",
    "    generate_input = st.chat_input(\"생성하고 싶은 이미지를 설명해주세요\")\n",
    "\n",
    "\n",
    "    if generate_input:\n",
    "\n",
    "\n",
    "        response = client.images.generate(\n",
    "                    model='dall-e-3',\n",
    "                    prompt=generate_input,\n",
    "                    size=\"1024x1024\",\n",
    "                    quality = 'standard',\n",
    "                    response_format='b64_json',\n",
    "                    n=1\n",
    "                )\n",
    "        result = response.model_dump()\n",
    "        img = response.data[0]\n",
    "        img_data = base64.b64decode(img.b64_json)\n",
    "        img = Image.open(io.BytesIO(img_data))\n",
    "        st.image(img)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
