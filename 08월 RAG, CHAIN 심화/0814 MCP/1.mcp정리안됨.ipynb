{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54034df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a66746",
   "metadata": {},
   "outputs": [],
   "source": [
    "- 캐글 노트북 gpu 버전 \n",
    "\n",
    "pip install genaibook\n",
    "\n",
    "\n",
    "import requests \n",
    "from PIL import Image\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from genaibook.core import SampleURL \n",
    "\n",
    "\n",
    "import torch \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cput'\n",
    "\n",
    "#clip 모델은 이미지를 인코딩하는 비전 모델, 텍스트를 인코딩하는 텍스트 모델델\n",
    "#CLIPProcessor는 텍스트 토크나이저와 같은 기능을 수행 (전처리)\n",
    "clip = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "url = SampleURL.LionExample\n",
    "image_inputs = processor(images=Image.open(requests.get(url, stream=True).raw), return_tensors='pt')\n",
    "pixel_values = image_inputs['pixel_values']\n",
    "\n",
    "\n",
    "print (pixel_values.shape)\n",
    "print (pixel_values.min())\n",
    "print (pixel_values.max())\n",
    "print (pixel_values.std())\n",
    "print (pixel_values.mean())\n",
    "\n",
    "\n",
    "\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "width, height = image.size \n",
    "crop_length = min(image.size)\n",
    "\n",
    "left = (width - crop_length) / 2\n",
    "top = (height - crop_length) / 2\n",
    "right = (width + crop_length) / 2\n",
    "bottom = (height + crop_length) / 2\n",
    "\n",
    "\n",
    "image.crop((left, top, right, bottom))\n",
    "#이미지에서 임베딩 벡터를 추출 \n",
    "with torch.inference_mode():\n",
    "    output = clip.vision_model(pixel_values.to(device))\n",
    "\n",
    "image_embedding = output.pooler_output\n",
    "\n",
    "\n",
    "#텍스트 임베딩\n",
    "prompts = [\n",
    "    'a photo of a lion', \n",
    "    'a photo of a zebra'\n",
    "]\n",
    "text_inputs = processor(text=prompts, return_tensors='pt', padding=True)\n",
    "\n",
    "text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "with torch.inference_mode():\n",
    "    text_ouputs = clip.text_model(**text_inputs)\n",
    "text_embeddings = text_ouputs.pooler_output\n",
    "\n",
    "\n",
    "print(clip.text_projection)\n",
    "print(clip.visual_projection)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    text_embeddings = clip.text_projection(text_embeddings)\n",
    "    image_embeddings= clip.visual_projection(image_embedding)\n",
    "\n",
    "#코사인 유사도 계산을 위한 단위크로 정규화 작업 \n",
    "text_embeddings_norm = text_embeddings / text_embeddings.norm(p=2, dim=1, keepdim=True)\n",
    "image_embeddings_norm = image_embeddings / image_embeddings.norm(p=2, dim=1, keepdim=True)\n",
    "\n",
    "similarities = torch.matmul(text_embeddings_norm,image_embeddings_norm.T)\n",
    "\n",
    "(similarities * 100).softmax(dim=0)\n",
    "제로샷 \n",
    "clip = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "\n",
    "image = Image.open(requests.get(SampleURL.LionExample, stream=True).raw)\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    \"a photo of a lion\",\n",
    "    \"a photo of a zebra\",\n",
    "    \"a photo of a cat\",\n",
    "    \"a photo of an adorable lion cub\",\n",
    "    \"a puppy\",\n",
    "    \"a lion behind a branch\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inputs = processor(\n",
    "    text=prompts, images=image, return_tensors=\"pt\", padding=True\n",
    ")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "\n",
    "ouputs = clip(**inputs)\n",
    "\n",
    "probabilities = ouputs.logits_per_image.softmax(dim=1)[0].cpu().detach().tolist()\n",
    "\n",
    "\n",
    "for prob, text in sorted(zip(probabilities, prompts), reverse=True):\n",
    "    print(f\"{100 * prob : 2.0f} : {text}\")\n",
    "\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-image-classification\",\n",
    "    model=\"openai/clip-vit-large-patch14\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "\n",
    "url = \"https://www.hollywoodreporter.com/wp-content/uploads/2025/07/KpopDemonHunters_ProRes422HQ_SDR_2ch_20250424.01_04_39_22-e1751568946588.jpg?crop=135px%2C0px%2C1433px%2C803px&resize=681%2C383\"\n",
    "\n",
    "demon = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "\n",
    "prompts = ['There are three ladies', 'There are three men'']\n",
    "scores = classifier(\n",
    "    demon,\n",
    "    candidate_labels=prompts,\n",
    "    hypothesis_template=\"{}\",\n",
    ")\n",
    "\n",
    "\n",
    "-------\n",
    "\n",
    "cmd  창 열고 \n",
    "\n",
    "\n",
    "mkdir clip\n",
    "\n",
    "cd clip \n",
    "\n",
    "uv init \n",
    "\n",
    "uv venv \n",
    "\n",
    ".venv\\Scripts\\activate\n",
    "\n",
    "uv add transformers torch pillow jupyter \n",
    "\n",
    "jupyter lab \n",
    "\n",
    "\n",
    "\n",
    "- 노트북 열고 \n",
    "\n",
    "from PIL import Image\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import requests \n",
    "\n",
    "import torch \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-image-classification\",\n",
    "    model=\"openai/clip-vit-large-patch14\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "demon = Image.open(\"./sleep.jpg\")\n",
    "\n",
    "prompts = ['A person is studying at a desk.', 'Someone is sleeping at a desk.', 'Someone is playing on the desk.']\n",
    "\n",
    "%time\n",
    "scores = classifier(\n",
    "    demon,\n",
    "    candidate_labels=prompts,\n",
    "    hypothesis_template=\"{}\",\n",
    ")\n",
    "\n",
    "scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prompts = ['He’s moving a stack of magazines', 'He’s attaching a notice to a board', 'He’s checking the time on his watch','He’s writing on a pad.']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-----\n",
    "- connect_to_server_and_run 함수 삭제 \n",
    "\n",
    "server_config.json\n",
    "\n",
    "{\n",
    "    \"mcpServers\": {\n",
    "       \n",
    "        \"filesystem\": {\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\n",
    "                \"-y\",\n",
    "                \"@modelcontextprotocol/server-filesystem\",\n",
    "                \".\"\n",
    "            ]\n",
    "        },\n",
    "        \"research\": {\n",
    "            \"command\": \"uv\",\n",
    "            \"args\": [\"run\", \"research_server.py\"]\n",
    "        },\n",
    "        \"fetch\": {\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\"mcp-server-fetch\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict, TypedDict\n",
    "from contextlib import AsyncExitStack\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "\n",
    "# 도구 정의를 위한 TypedDict\n",
    "class ToolDefinition(TypedDict):\n",
    "    name: str\n",
    "    description: str\n",
    "    input_schema: dict\n",
    "\n",
    "\n",
    "class MCP_ChatBot:\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        # 여러 MCP 세션 관리\n",
    "        self.sessions: List[ClientSession] = []\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        self.available_tools: List[ToolDefinition] = []\n",
    "       \n",
    "        # 도구 이름 ↔ 해당 도구를 제공하는 세션 매핑\n",
    "        self.tool_to_session: Dict[str, ClientSession] = {}\n",
    "\n",
    "\n",
    "    async def connect_to_server(self, server_name: str, server_config: dict) -> None:\n",
    "        \"\"\"단일 MCP 서버에 연결\"\"\"\n",
    "        try:\n",
    "            params = StdioServerParameters(**server_config)\n",
    "            # stdio transport 생성\n",
    "            read, write = await self.exit_stack.enter_async_context(stdio_client(params))\n",
    "            session = await self.exit_stack.enter_async_context(ClientSession(read, write))\n",
    "            await session.initialize()\n",
    "            self.sessions.append(session)\n",
    "\n",
    "\n",
    "            # 도구 목록 조회\n",
    "            response = await session.list_tools()\n",
    "            print(f\"\\nConnected to {server_name} with tools:\", [t.name for t in response.tools])\n",
    "            for tool in response.tools:\n",
    "                # 세션과 도구 매핑\n",
    "                self.tool_to_session[tool.name] = session\n",
    "                self.available_tools.append({\n",
    "                    \"name\": tool.name,\n",
    "                    \"description\": tool.description,\n",
    "                    \"input_schema\": tool.inputSchema\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to connect to {server_name}: {e}\")\n",
    "\n",
    "\n",
    "    async def connect_to_servers(self) -> None:\n",
    "        \"\"\"설정 파일(server_config.json)에 정의된 모든 서버에 연결\"\"\"\n",
    "        try:\n",
    "            with open(\"server_config.json\", \"r\") as f:\n",
    "                cfg = json.load(f)\n",
    "            for name, params in cfg.get(\"mcpServers\", {}).items():\n",
    "                await self.connect_to_server(name, params)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading server configuration: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    async def process_query(self, query: str) -> None:\n",
    "        # 사용자 메시지로 대화 시작\n",
    "        messages = [{\"role\": \"user\", \"content\": query}]\n",
    "        # OpenAI 함수 스펙으로 도구 정의\n",
    "        functions = [\n",
    "            {\"name\": t[\"name\"], \"description\": t[\"description\"], \"parameters\": t[\"input_schema\"]}\n",
    "            for t in self.available_tools\n",
    "        ]\n",
    "\n",
    "\n",
    "        while True:\n",
    "            # OpenAI 함수 호출 자동화\n",
    "            resp = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=messages,\n",
    "                functions=functions,\n",
    "                function_call=\"auto\",\n",
    "                max_tokens=2024,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            msg = resp.choices[0].message\n",
    "\n",
    "\n",
    "            # 도구 호출 요청 처리\n",
    "            if getattr(msg, \"function_call\", None):\n",
    "                name = msg.function_call.name\n",
    "                args = json.loads(msg.function_call.arguments)\n",
    "                print(f\"Calling tool {name} with args {args}\")\n",
    "                session = self.tool_to_session[name]\n",
    "                result = await session.call_tool(name, arguments=args)\n",
    "                # 함수 결과를 대화에 추가\n",
    "                messages.append({\"role\": \"function\", \"name\": name, \"content\": result.content})\n",
    "                # 반복하여 후속 응답 처리\n",
    "                continue\n",
    "\n",
    "\n",
    "            # 일반 응답 출력 및 종료\n",
    "            print(msg.content)\n",
    "            messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
    "            break\n",
    "\n",
    "\n",
    "    async def chat_loop(self) -> None:\n",
    "        print(\"\\nMCP Chatbot Started! Type queries or 'quit' to exit.\")\n",
    "        while True:\n",
    "            query = input(\"Query: \").strip()\n",
    "            if query.lower() == 'quit':\n",
    "                break\n",
    "            await self.process_query(query)\n",
    "            print()\n",
    "\n",
    "\n",
    "    async def cleanup(self) -> None:\n",
    "        \"\"\"모든 리소스 정리\"\"\"\n",
    "        await self.exit_stack.aclose()\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "    nest_asyncio.apply()\n",
    "    chatbot = MCP_ChatBot()\n",
    "    try:\n",
    "        await chatbot.connect_to_servers()\n",
    "        await chatbot.chat_loop()\n",
    "    finally:\n",
    "        await chatbot.cleanup()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "uv run mcp_client2.py \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
