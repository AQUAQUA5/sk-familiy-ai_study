{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba394b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a1bb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('mongodb://admin:admin@222.112.208.67:27017')\n",
    "db = client['news']\n",
    "documents = db['contents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "711817ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Okt()\n",
    "\n",
    "stop_word = ['기자', '뉴시스', '[', ']', '(', ')', '했다', '에서', '.[', '하는']\n",
    "contents_list =  [  tokenizer.nouns(docu['content'] )  for docu in documents.find()]\n",
    "\n",
    "tokens = [[   t  for t in tok if t not in stop_word and len(t) > 1 ]   for tok in contents_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a7188cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tmp = []\n",
    "for tok in contents_list: \n",
    "    tmp = []\n",
    "    for t in tok:\n",
    "        if t not in stop_word and len(t) > 1:\n",
    "            tmp.append(t)\n",
    "    tokens_tmp.append(tmp)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58753539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b4e07c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "    counter = Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    vocab = special_tokens\n",
    "    for token, count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "759fe0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(corpus=tokens, n_vocab=5000, special_tokens= [\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea18cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = { token : idx  for idx, token in enumerate(vocab)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18ac3529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_id['서울']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd5c806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_token = {idx : token for idx, token in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ac84509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_pairs(tokens, window_size):\n",
    "    pairs = []\n",
    "    for sentence in tokens:\n",
    "        sentense_length = len(sentence)\n",
    "        for idx, center_word in enumerate(sentence):\n",
    "            window_start = max(0, idx-window_size)\n",
    "            window_end = min(sentense_length, idx+window_size + 1)\n",
    "            center_word = sentence[idx]\n",
    "            context_words = sentence[window_start:idx] + sentence[idx+1:window_end]\n",
    "            for context_word in context_words:\n",
    "                pairs.append([center_word, context_word])\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c11b3e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['서울', '지난', '방송', '예능', '우리', '새끼', '이상민', '신혼집', '공개', '사진']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4f6e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = get_word_pairs(tokens=tokens, window_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "678f7bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_pairs(word_pairs, token_to_id):\n",
    "    pairs = [] \n",
    "    unk_index = token_to_id['<unk>']\n",
    "    for center_word, context_word in word_pairs:\n",
    "        center_idx = token_to_id.get(center_word, unk_index )\n",
    "        context_idx = token_to_id.get(context_word, unk_index)\n",
    "        pairs.append([center_idx, context_idx])\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37ce98b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_pairs = get_index_pairs(word_pairs, token_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b8d5b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch : 2.7.1+cu128\n",
      "CUDA : 12.8\n",
      "cuDNN : 90701\n",
      "GPU 사용 가능!\n",
      "사용 가능한 GPU 개수: 1\n",
      "첫 번째 GPU 이름: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('Torch :', torch.__version__)\n",
    "print( 'CUDA :' , torch.version.cuda)\n",
    "print('cuDNN :',torch.backends.cudnn.version())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU 사용 가능!\")\n",
    "    print(f\"사용 가능한 GPU 개수: {torch.cuda.device_count()}\")\n",
    "    print(f\"첫 번째 GPU 이름: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU를 사용할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3a8727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings = vocab_size,\n",
    "            embedding_dim = embedding_dim\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features =  embedding_dim,\n",
    "            out_features = vocab_size\n",
    "        )\n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        output = self.linear(embeddings)\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1343acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "index_pairs = torch.tensor(index_pairs)\n",
    "center_indexs = index_pairs[:, 0]\n",
    "context_indexs = index_pairs[:, 1]\n",
    "\n",
    "dataset = TensorDataset(center_indexs, context_indexs)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5dd34b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "word2vec = SkipGram(vocab_size=len(token_to_id), embedding_dim = 128).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(word2vec.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f03ff1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cce00480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1 Cost : 7.057\n",
      "Epoch    2 Cost : 6.300\n",
      "Epoch    3 Cost : 6.072\n",
      "Epoch    4 Cost : 5.941\n",
      "Epoch    5 Cost : 5.847\n",
      "Epoch    6 Cost : 5.774\n",
      "Epoch    7 Cost : 5.712\n",
      "Epoch    8 Cost : 5.660\n",
      "Epoch    9 Cost : 5.613\n",
      "Epoch   10 Cost : 5.571\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    cost = 0.0 \n",
    "    for  input_ids, target_ids in   dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "        logits = word2vec(input_ids)\n",
    "        loss = criterion(logits, target_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cost += loss \n",
    "    cost = cost / len(dataloader)\n",
    "    print (f\"Epoch {epoch+1:4d} Cost : {cost:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38cf188e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28108668"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from numpy.linalg import norm\n",
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (norm(b) * norm(a))\n",
    "\n",
    "김지민 = word2vec.embedding.weight[token_to_id['김지민']].detach().cpu().numpy()\n",
    "김준호 = word2vec.embedding.weight[token_to_id['김준호']].detach().cpu().numpy()\n",
    "\n",
    "cosine_sim(김지민, 김준호)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c17c665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "김지민와 가장 유사한 5 개 단어\n",
      "결혼식 - 유사도 : 0.2964\n",
      "김대희 - 유사도 : 0.2946\n",
      "김준호 - 유사도 : 0.2811\n",
      "자료 - 유사도 : 0.2803\n",
      "백지영 - 유사도 : 0.2753\n",
      "이티 - 유사도 : 0.2601\n",
      "뮤직뱅크 - 유사도 : 0.2597\n",
      "면모 - 유사도 : 0.2532\n",
      "최강 - 유사도 : 0.2516\n",
      "엑스포츠 - 유사도 : 0.2499\n",
      "플래닛 - 유사도 : 0.2487\n",
      "단절 - 유사도 : 0.2472\n",
      "예상 - 유사도 : 0.2470\n",
      "열애 - 유사도 : 0.2458\n",
      "경악 - 유사도 : 0.2448\n",
      "개월 - 유사도 : 0.2446\n",
      "연하 - 유사도 : 0.2408\n",
      "냉부 - 유사도 : 0.2390\n",
      "홈즈 - 유사도 : 0.2375\n",
      "방문 - 유사도 : 0.2320\n"
     ]
    }
   ],
   "source": [
    "token_to_embedding = dict()\n",
    "embedding_matrix = word2vec.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "for word, embedding in zip(vocab, embedding_matrix):\n",
    "    token_to_embedding[word] = embedding\n",
    "token = \"김지민\"\n",
    "token_embedding = token_to_embedding[token]\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    cosine = np.dot(b, a) / (norm(b, axis=1) * norm(a))\n",
    "    return cosine\n",
    "\n",
    "def top_n_index(cosine_matrix, n):\n",
    "    closest_indexes = cosine_matrix.argsort()[::-1]\n",
    "    top_n = closest_indexes[1 : n + 1]\n",
    "    return top_n\n",
    "\n",
    "cosine_matrix = cosine_similarity(token_embedding, embedding_matrix)\n",
    "top_n = top_n_index(cosine_matrix, n=20)\n",
    "\n",
    "print(f\"{token}와 가장 유사한 5 개 단어\")\n",
    "for index in top_n:\n",
    "    print(f\"{id_to_token[index]} - 유사도 : {cosine_matrix[index]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6770aac2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m word2vec = Word2Vec(sentences=\u001b[43mtokens\u001b[49m, vector_size=\u001b[32m128\u001b[39m, \n\u001b[32m      3\u001b[39m         window=\u001b[32m5\u001b[39m, min_count=\u001b[32m3\u001b[39m, sg=\u001b[32m1\u001b[39m, epochs=\u001b[32m10\u001b[39m, max_final_vocab= \u001b[32m5000\u001b[39m\n\u001b[32m      4\u001b[39m         )\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# word2vec.wv['김지민']\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# word2vec.wv.most_similar(\"김지민\", topn=10)\u001b[39;00m\n\u001b[32m      7\u001b[39m word2vec.wv.similarity(w1=\u001b[33m'\u001b[39m\u001b[33m김준호\u001b[39m\u001b[33m'\u001b[39m, w2=\u001b[33m'\u001b[39m\u001b[33m김지민\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec = Word2Vec(sentences=tokens, vector_size=128, \n",
    "        window=5, min_count=3, sg=1, epochs=10, max_final_vocab= 5000\n",
    "        )\n",
    "# word2vec.wv['김지민']\n",
    "# word2vec.wv.most_similar(\"김지민\", topn=10)\n",
    "word2vec.wv.similarity(w1='김준호', w2='김지민')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99b87c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace86875",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sk2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
