{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "492ad604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arxiv.org/abs/2412.18022v1\n",
      "http://arxiv.org/abs/2406.10300v1\n",
      "http://arxiv.org/abs/2405.19888v1\n",
      "http://arxiv.org/abs/2311.10372v2\n",
      "http://arxiv.org/abs/2411.15764v1\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "\n",
    "client = arxiv.Client()\n",
    "topic = 'llm'\n",
    "search = arxiv.Search(\n",
    "    query= topic,\n",
    "    max_results=5,\n",
    "    sort_by= arxiv.SortCriterion.Relevance\n",
    ")\n",
    "papers = client.results(search=search)\n",
    "\n",
    "for paper in papers:\n",
    "    print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2f52bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from typing import List\n",
    "PAPER_DIR = \"./papers\"\n",
    "def search_papers(topic: str, max_results: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    주제를 기반으로 arXiv에서 논문을 검색하고 해당 정보를 저장한다.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        topic: 검색할 주제\n",
    "        max_results: 검색할 최대 결과 수 (기본값: 5)\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        검색에서 찾은 논문 ID 목록\n",
    "    \"\"\"\n",
    "    # arxiv를 사용하여 논문 찾기\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "        query     = topic,\n",
    "        max_results = max_results,\n",
    "        sort_by   = arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    papers = client.results(search)\n",
    "\n",
    "\n",
    "    # 이 주제에 대한 디렉토리 생성\n",
    "    path = os.path.join(PAPER_DIR, topic.lower().replace(\" \", \"_\"))\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "    file_path = os.path.join(path, \"papers_info.json\")\n",
    "\n",
    "\n",
    "    # 기존 논문 정보 로드 시도\n",
    "    try:\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            papers_info = json.load(json_file)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        papers_info = {}\n",
    "\n",
    "\n",
    "    # 각 논문을 처리하고 papers_info에 추가\n",
    "    paper_ids = []\n",
    "    for paper in papers:\n",
    "        paper_id   = paper.get_short_id()\n",
    "        paper_info = {\n",
    "            \"title\"    : paper.title,\n",
    "            \"authors\"  : [author.name for author in paper.authors],\n",
    "            \"summary\"  : paper.summary,\n",
    "            \"pdf_url\"  : paper.pdf_url,\n",
    "            \"published\": str(paper.published.date())\n",
    "        }\n",
    "        paper_ids.append(paper_id)\n",
    "        papers_info[paper_id] = paper_info\n",
    "\n",
    "\n",
    "    # 업데이트된 papers_info를 json 파일에 저장\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(papers_info, json_file, indent=2)\n",
    "\n",
    "\n",
    "    print(f\"결과가 다음 위치에 저장되었습니다: {file_path}\")\n",
    "    return paper_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a00660c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(paper_id: str) -> str:\n",
    "    \"\"\"\n",
    "    모든 주제 디렉토리에서 특정 논문에 관한 정보를 검색한다.\n",
    "   \n",
    "    Args:\n",
    "        paper_id: 검색할 논문의 ID\n",
    "       \n",
    "    Returns:\n",
    "        찾은 경우 논문 정보가 담긴 JSON 문자열, 찾지 못한 경우 오류 메시지\n",
    "    \"\"\"\n",
    " \n",
    "    for item in os.listdir(PAPER_DIR):\n",
    "        item_path = os.path.join(PAPER_DIR, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            file_path = os.path.join(item_path, \"papers_info.json\")\n",
    "            if os.path.isfile(file_path):\n",
    "                try:\n",
    "                    with open(file_path, \"r\") as json_file:\n",
    "                        papers_info = json.load(json_file)\n",
    "                        if paper_id in papers_info:\n",
    "                            return json.dumps(papers_info[paper_id], indent=2)\n",
    "                except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "                    print(f\"{file_path} 읽기 오류: {str(e)}\")\n",
    "                    continue\n",
    "   \n",
    "    return f\"논문 {paper_id}와 관련된 저장된 정보가 없다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33c53e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과가 다음 위치에 저장되었습니다: ./papers/llm/papers_info.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2412.18022v1',\n",
       " '2406.10300v1',\n",
       " '2405.19888v1',\n",
       " '2311.10372v2',\n",
       " '2411.15764v1']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_papers('llm', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "304f35a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"title\": \"Trustworthy and Efficient LLMs Meet Databases\",\\n  \"authors\": [\\n    \"Kyoungmin Kim\",\\n    \"Anastasia Ailamaki\"\\n  ],\\n  \"summary\": \"In the rapidly evolving AI era with large language models (LLMs) at the core,\\\\nmaking LLMs more trustworthy and efficient, especially in output generation\\\\n(inference), has gained significant attention. This is to reduce plausible but\\\\nfaulty LLM outputs (a.k.a hallucinations) and meet the highly increased\\\\ninference demands. This tutorial explores such efforts and makes them\\\\ntransparent to the database community. Understanding these efforts is essential\\\\nin harnessing LLMs in database tasks and adapting database techniques to LLMs.\\\\nFurthermore, we delve into the synergy between LLMs and databases, highlighting\\\\nnew opportunities and challenges in their intersection. This tutorial aims to\\\\nshare with database researchers and practitioners essential concepts and\\\\nstrategies around LLMs, reduce the unfamiliarity of LLMs, and inspire joining\\\\nin the intersection between LLMs and databases.\",\\n  \"pdf_url\": \"http://arxiv.org/pdf/2412.18022v1\",\\n  \"published\": \"2024-12-23\"\\n}'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_info('2412.18022v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b869d197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과가 다음 위치에 저장되었습니다: ./papers/model_compression_using_sparse_matrices/papers_info.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2105.11025v1',\n",
       " '2102.07071v1',\n",
       " '2210.11420v1',\n",
       " '2506.04208v1',\n",
       " '1905.07931v1',\n",
       " '1207.2079v1',\n",
       " '1312.0525v2',\n",
       " '2409.08699v2',\n",
       " '1707.08208v1',\n",
       " '2303.16106v1']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_papers('Model Compression using Sparse Matrices', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1a0c383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"title\": \"Compressing Heavy-Tailed Weight Matrices for Non-Vacuous Generalization Bounds\",\\n  \"authors\": [\\n    \"John Y. Shin\"\\n  ],\\n  \"summary\": \"Heavy-tailed distributions have been studied in statistics, random matrix\\\\ntheory, physics, and econometrics as models of correlated systems, among other\\\\ndomains. Further, heavy-tail distributed eigenvalues of the covariance matrix\\\\nof the weight matrices in neural networks have been shown to empirically\\\\ncorrelate with test set accuracy in several works (e.g. arXiv:1901.08276), but\\\\na formal relationship between heavy-tail distributed parameters and\\\\ngeneralization bounds was yet to be demonstrated. In this work, the compression\\\\nframework of arXiv:1802.05296 is utilized to show that matrices with heavy-tail\\\\ndistributed matrix elements can be compressed, resulting in networks with\\\\nsparse weight matrices. Since the parameter count has been reduced to a sum of\\\\nthe non-zero elements of sparse matrices, the compression framework allows us\\\\nto bound the generalization gap of the resulting compressed network with a\\\\nnon-vacuous generalization bound. Further, the action of these matrices on a\\\\nvector is discussed, and how they may relate to compression and resilient\\\\nclassification is analyzed.\",\\n  \"pdf_url\": \"http://arxiv.org/pdf/2105.11025v1\",\\n  \"published\": \"2021-05-23\"\\n}'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_info('2105.11025v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30c6f696",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"search_papers\",\n",
    "        \"description\": \"주제를 기반으로 arXiv에서 논문을 검색하고 해당 정보를 저장한다.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"topic\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"검색할 주제\"\n",
    "                },\n",
    "                \"max_results\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"검색할 최대 결과 수\",\n",
    "                    \"default\": 5\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"topic\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"extract_info\",\n",
    "        \"description\": \"모든 주제 디렉토리에서 특정 논문에 관한 정보를 검색한다.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"paper_id\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"검색할 논문의 ID\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"paper_id\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "mapping_tool_function  = {\n",
    "    \"search_papers\" : search_papers,\n",
    "    \"extract_info\"  : extract_info\n",
    "}\n",
    "\n",
    "\n",
    "def execute_tool(tool_name, tool_args):\n",
    "    result = mapping_tool_function[tool_name](**tool_args)\n",
    "\n",
    "\n",
    "    if result is None:\n",
    "        return \"작업이 완료되었찌만 결과는 반환하지 않았다\"\n",
    "   \n",
    "    if isinstance(result, list):\n",
    "        return \", \".join(result)\n",
    "   \n",
    "    if isinstance(result, dict):\n",
    "        return json.dumps(result, indent=2, ensure_ascii=False)\n",
    "   \n",
    "    return str(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8bdc670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "messages = [{'role' : 'user', 'content' : \"llm에 대해서 알려줘\"}]\n",
    "resp  = client.chat.completions.create(\n",
    "    model='gpt-4-0125-preview',\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call='auto',\n",
    "    max_tokens=2024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6314ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.choices[0].message\n",
    "\n",
    "msg = resp.choices[0].message\n",
    "call = msg.function_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e752a102",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_result = execute_tool(call.name,json.loads(call.arguments) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef939b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query: str):\n",
    "    messages = [{\"role\": \"user\", \"content\": query}]\n",
    "\n",
    "    while True:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            functions=functions,\n",
    "            function_call=\"auto\",\n",
    "            max_tokens=2024\n",
    "        )\n",
    "        msg = resp.choices[0].message\n",
    "\n",
    "        # 1) 일반 텍스트 응답\n",
    "        if msg.content is not None:\n",
    "            print(msg.content)\n",
    "            break\n",
    "\n",
    "        # 2) 함수 호출 응답\n",
    "        call = msg.function_call\n",
    "        fname = call.name\n",
    "        fargs = json.loads(call.arguments)\n",
    "\n",
    "        print(f\"Calling tool `{fname}` with args: {fargs}\")\n",
    "        tool_result = execute_tool(fname, fargs)\n",
    "\n",
    "        # assistant의 함수 호출 메시지를 기록\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"function_call\": {\n",
    "                \"name\": fname,\n",
    "                \"arguments\": call.arguments\n",
    "            }\n",
    "        })\n",
    "        # 함수 실행 결과를 function 역할로 추가\n",
    "        messages.append({\n",
    "            \"role\": \"function\",\n",
    "            \"name\": fname,\n",
    "            \"content\": tool_result\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf7813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_loop():\n",
    "    print(\"쿼리를 입력하거나 'quit'를 입력해 종료합니다.\")\n",
    "    while True:\n",
    "        query = input(\"Query: \").strip()\n",
    "        if query.lower() == \"quit\":\n",
    "            break\n",
    "        try:\n",
    "            process_query(query)\n",
    "       \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
